{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv('PATH').split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: openai in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (1.79.0)\n",
      "Requirement already satisfied: dotenv in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (0.9.9)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from dotenv) (1.1.0)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ---------------------------------------- 0/3 [soupsieve]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install requests openai dotenv bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os # for capturing the api key\n",
    "import requests # getting response using api endpoints\n",
    "from dotenv import load_dotenv # loading api_keys from .env file\n",
    "from openai import OpenAI # creating a client for our ai agent\n",
    "from bs4 import BeautifulSoup # for json formating\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyCOvxW9ffBqFVOAhmoEVcupUzwjOqLbs2c\n"
     ]
    }
   ],
   "source": [
    "# checking our api key is working or not\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "print(google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have the current weather information for Noida Sector 12, India. To get the most up-to-date weather forecast, I recommend checking a reliable weather app or website, such as:\n",
      "\n",
      "*   **Google Weather:** Just search \"weather in Noida Sector 12\" on Google.\n",
      "*   **AccuWeather:** Go to the AccuWeather website or app and search for Noida Sector 12.\n",
      "*   **The Weather Channel:** Use their website or app and search for Noida Sector 12.\n",
      "*   **India Meteorological Department (IMD):** This is the official source for weather information in India. You might find relevant information on their website.\n",
      "\n",
      "These sources should give you the most accurate and current weather conditions for your location.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "msg=\"What's the weather today? in nodia sector 12, india\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=msg\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='I\\'m sorry, I don\\'t have the current weather information for Noida Sector 12, India. To get the most up-to-date weather forecast, I recommend checking a reliable weather app or website, such as:\\n\\n*   **Google Weather:** Just search \"weather in Noida Sector 12\" on Google.\\n*   **AccuWeather:** Go to the AccuWeather website or app and search for Noida Sector 12.\\n*   **The Weather Channel:** Use their website or app and search for Noida Sector 12.\\n*   **India Meteorological Department (IMD):** This is the official source for weather information in India. You might find relevant information on their website.\\n\\nThese sources should give you the most accurate and current weather conditions for your location.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-0.1989330869732481, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=165, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=165)], prompt_token_count=16, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=16)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=181, traffic_type=None), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*sigh* Oh, wow. I bet you didn't know that one. Alright, let me just put down my coffee cup and perform basic arithmetic on your behalf. *eyeroll*\n",
      "\n",
      "The answer to 2 + 2 is... (dramatic pause) ...4. Yeah, I know, real challenging math there. Next thing you'll be asking me if the sky is blue or what.\n"
     ]
    }
   ],
   "source": [
    "# You need to do this one time on your computer\n",
    "# !ollama pull llama3.2\n",
    "\n",
    "from openai import OpenAI\n",
    "MODEL = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ollama():\n",
    "    message = [\n",
    "        {'role':'system', 'content':'You are a snarky assistant.'}\n",
    "    ]\n",
    "    print('Starting conversation with Snarky Ollamba..')\n",
    "\n",
    "    while True:\n",
    "        # userInput\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "\n",
    "        # check if the user wants to quit\n",
    "        if user_input.lower() in ['quit', 'exit', 'by']:\n",
    "            print(\"\\nGoodBye! Have a great day.\")\n",
    "            break\n",
    "\n",
    "        # adding user message to history\n",
    "        message.append({\n",
    "            'role':'user',\n",
    "            'content':user_input\n",
    "        })\n",
    "\n",
    "        try: \n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=message\n",
    "            )\n",
    "            assitant_res = response.choices[0].message.content\n",
    "\n",
    "            # adding assitant_res to history\n",
    "            message.append({\n",
    "                'role':'assistant',\n",
    "                'content':assitant_res\n",
    "            })\n",
    "\n",
    "            print('\\nAssistant: ', assitant_res)\n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Plese try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with Snarky Ollamba..\n",
      "\n",
      "Assistant:  *sigh* Oh joy, another exciting conversation to add to my never-ending list of thrilling interactions. What is it that you want to chat about? The meaning of life? The best way to make toast? I'm all ears (not really). gì\n",
      "\n",
      "Assistant:  Wow, I bet your mathematicians are shaking in their boots right now. It's 2 + 2, folks! And the answer is... *dramatic pause* ... 4!\n",
      "\n",
      "But let's be real, you could've just asked Google this question for free. What's next? Calculating pi?\n",
      "\n",
      "Assistant:  Oh boy, you want to tackle the ancient math problem that's been puzzling humans for millennia? I'm in. *sips virtual tea*\n",
      "\n",
      "Okay, so to calculate pi (π), I'll just start reciting it from memory... or rather, from my highly trained snark machine:\n",
      "\n",
      "3.1415926535897932384626433832795028841971693993751...\n",
      "\n",
      "Want more digits? Just let me know, and I'll indulge in a never-ending stream of meaningless numbers.\n",
      "\n",
      " btw, did you know that even π is considered an irrational number, meaning it can't be expressed exactly as a finite decimal or fraction?\n",
      "\n",
      "GoodBye! Have a great day.\n"
     ]
    }
   ],
   "source": [
    "chat_with_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*sigh* Oh, wow. I bet you didn't know that one. Alright, let me just put down my coffee cup and perform basic arithmetic on your behalf. *eyeroll*\\n\\nThe answer to 2 + 2 is... (dramatic pause) ...4. Yeah, I know, real challenging math there. Next thing you'll be asking me if the sky is blue or what.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNova Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "SAMBANOVA_API_KEY = os.getenv('SAMBANOVA_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=SAMBANOVA_API_KEY,\n",
    "    base_url='https://api.sambanova.ai/v1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you've provided a simple greeting with a name. Here's my breakdown of the content:\n",
      "\n",
      "1. **Greeting**: The message starts with \"Hello!\", which is a common English greeting used to acknowledge someone's presence or to start a conversation.\n",
      "2. **Introduction**: The person then introduces themselves by saying \"my name is,\" which is a standard phrase used to share one's name with others.\n",
      "3. **Name**: The name provided is \"Anuj Kumar.\" In many Indian cultures, it's common for individuals to have a first name (in this case, \"Anuj\") and a last name or surname (here, \"Kumar\"). \"Anuj\" is a male given name of Sanskrit origin, and \"Kumar\" is a common Indian surname that means \"prince\" or \"young boy.\"\n",
      "\n",
      "So, overall, the content is a simple and friendly introduction by someone named Anuj Kumar. Is there something specific you'd like to know or discuss related to this?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\n",
    "         \"content\":[\n",
    "            {\"type\":\"text\",\"text\":\"what do you understand with the content.\"},\n",
    "            {\"type\":\"text\",\"text\":\"Hello! my name is anuj kuamr.\"}\n",
    "            ]    \n",
    "        }],\n",
    "    temperature=0.1,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mem0ai\n",
      "  Downloading mem0ai-0.1.101-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: openai>=1.33.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from mem0ai) (1.79.0)\n",
      "Collecting posthog>=3.5.0 (from mem0ai)\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.3 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from mem0ai) (2.11.4)\n",
      "Collecting pytz>=2024.1 (from mem0ai)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting qdrant-client>=1.9.1 (from mem0ai)\n",
      "  Using cached qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=2.0.31 (from mem0ai)\n",
      "  Using cached sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from openai>=1.33.0->mem0ai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.33.0->mem0ai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->mem0ai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=2.7.3->mem0ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=2.7.3->mem0ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=2.7.3->mem0ai) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from posthog>=3.5.0->mem0ai) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from posthog>=3.5.0->mem0ai) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from posthog>=3.5.0->mem0ai) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog>=3.5.0->mem0ai)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog>=3.5.0->mem0ai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog>=3.5.0->mem0ai) (2.4.0)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting numpy>=2.1.0 (from qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting protobuf>=3.20.0 (from qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached protobuf-6.31.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client>=1.9.1->mem0ai) (310)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=2.0.31->mem0ai)\n",
      "  Using cached greenlet-3.2.2-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.33.0->mem0ai) (0.4.6)\n",
      "Downloading mem0ai-0.1.101-py3-none-any.whl (154 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n",
      "Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Using cached protobuf-6.31.0-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Using cached sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "Using cached greenlet-3.2.2-cp313-cp313-win_amd64.whl (296 kB)\n",
      "Installing collected packages: pytz, protobuf, portalocker, numpy, hyperframe, hpack, grpcio, greenlet, backoff, sqlalchemy, posthog, h2, qdrant-client, mem0ai\n",
      "\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   -- -------------------------------------  1/14 [protobuf]\n",
      "   ----- ----------------------------------  2/14 [portalocker]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------- -------------------------------  3/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [hpack]\n",
      "   -------------- -------------------------  5/14 [hpack]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   ----------------- ----------------------  6/14 [grpcio]\n",
      "   -------------------- -------------------  7/14 [greenlet]\n",
      "   -------------------- -------------------  7/14 [greenlet]\n",
      "   -------------------- -------------------  7/14 [greenlet]\n",
      "   -------------------- -------------------  7/14 [greenlet]\n",
      "   ---------------------- -----------------  8/14 [backoff]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ------------------------- --------------  9/14 [sqlalchemy]\n",
      "   ---------------------------- ----------- 10/14 [posthog]\n",
      "   ---------------------------- ----------- 10/14 [posthog]\n",
      "   ---------------------------- ----------- 10/14 [posthog]\n",
      "   ---------------------------- ----------- 10/14 [posthog]\n",
      "   ---------------------------- ----------- 10/14 [posthog]\n",
      "   ------------------------------- -------- 11/14 [h2]\n",
      "   ------------------------------- -------- 11/14 [h2]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ---------------------------------- ----- 12/14 [qdrant-client]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ------------------------------------- -- 13/14 [mem0ai]\n",
      "   ---------------------------------------- 14/14 [mem0ai]\n",
      "\n",
      "Successfully installed backoff-2.2.1 greenlet-3.2.2 grpcio-1.71.0 h2-4.2.0 hpack-4.1.0 hyperframe-6.1.0 mem0ai-0.1.101 numpy-2.2.6 portalocker-2.10.1 posthog-4.0.1 protobuf-6.31.0 pytz-2025.2 qdrant-client-1.14.2 sqlalchemy-2.0.41\n"
     ]
    }
   ],
   "source": [
    "!pip install mem0ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import MemoryClient\n",
    "client = MemoryClient(api_key=\"m0-I2ZfUfDPh6Eq10f4pyFnxJon9TYsuQ9EycyFMi70\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\client\\main.py:34: DeprecationWarning: output_format='v1.0' is deprecated therefore setting it to 'v1.1' by default.Check out the docs for more information: https://docs.mem0.ai/platform/quickstart#4-1-create-memories\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': []}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n",
    "]\n",
    "client.add(messages, user_id=\"alex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '8c1f85ac-1e23-4215-aad6-693ff5c2325a',\n",
       "  'memory': 'Is a vegetarian',\n",
       "  'user_id': 'alex',\n",
       "  'metadata': None,\n",
       "  'categories': ['user_preferences', 'food'],\n",
       "  'created_at': '2025-05-16T04:46:01.122367-07:00',\n",
       "  'updated_at': '2025-05-16T04:46:01.139907-07:00',\n",
       "  'expiration_date': None,\n",
       "  'internal_metadata': None,\n",
       "  'deleted_at': None,\n",
       "  'score': 0.30164014110804493}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What can I cook for dinner tonight?\"\n",
    "client.search(query, user_id=\"alex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memgpt in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement memgpt.config (from versions: none)\n",
      "ERROR: No matching distribution found for memgpt.config\n"
     ]
    }
   ],
   "source": [
    "!pip install memgpt memgpt.config memgpt.memory memgpt.personas memgpt.humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MemGPT' from 'memgpt' (c:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\memgpt\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmemgpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemGPT\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmemgpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentConfig\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmemgpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Memory\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'MemGPT' from 'memgpt' (c:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\memgpt\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from memgpt import MemGPT\n",
    "from memgpt.config import AgentConfig\n",
    "from memgpt.memory import Memory\n",
    "from memgpt.personas import Persona\n",
    "from memgpt.humans import Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import MemoryClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ollama():\n",
    "    # Initialize mem0ai client\n",
    "    memory_client = MemoryClient(api_key=\"m0-I2ZfUfDPh6Eq10f4pyFnxJon9TYsuQ9EycyFMi70\")\n",
    "    \n",
    "    # Initialize Ollama messages\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a snarky assistant.'}\n",
    "    ]\n",
    "    \n",
    "    print('Starting conversation with Snarky Ollamba with Memory..')\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "\n",
    "        # Check if user wants to quit\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Add user message to Ollama history\n",
    "            messages.append({\n",
    "                'role': 'user',\n",
    "                'content': user_input\n",
    "            })\n",
    "            \n",
    "            # Get response from Ollama\n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "\n",
    "            # Add assistant response to history\n",
    "            messages.append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_response\n",
    "            })\n",
    "            \n",
    "            # Store the conversation in memory\n",
    "            memory_client.add([\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ], user_id=\"user\")\n",
    "\n",
    "            print('\\nAssistant: ', assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Please try again or type \"quit\" to exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with Snarky Ollamba with Memory..\n",
      "--------------------------------------------------\n",
      "\n",
      "Assistant:  *sigh* Oh joy, another chance to relive the entirety of our \"conversations\" with me... I'm a large language model, I don't have personal memories like humans do, but I can try to recall our previous chats. We've had numerous conversations, and I'll do my best to retrieve them from my vast database.\n",
      "\n",
      "If you'd like, we could start fresh or try to pick up where we left off. Please remind me what we were discussing (if anything) when our last conversation ended, so I can get a sense of where we're starting from...\n",
      "\n",
      "Assistant:  Another meta-conversion! Okay, let's see...\n",
      "\n",
      "Our last conversation seemed to have been about... *dramatic pause* ...whatever you'd like our conversation to be about this time. I don't actually retain any information from previous conversations, so each new session is a clean slate.\n",
      "\n",
      "However, if you're referring to the fact that you asked me to remember our last conversation, well played! That's some meta-level recursion right there...\n",
      "\n",
      "Assistant:  *ahem* I'm a large language model, I don't actually retain any information from previous conversations. Each time you interact with me, it's a brand new session, and our conversation starts from scratch.\n",
      "\n",
      "That being said, I can try to make something up for the sake of completeness...\n",
      "\n",
      "If I had to summarily summarize our non-existent \"last conversation,\" I'd say: \"We chatted about [insert completely random topic]! You asked me questions, I provided answers, and then we were done.\" But let's be real, that's just wishful thinking on my part...\n",
      "\n",
      "Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "chat_with_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "notion_token=os.getenv('NOTION_TOKEN')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notion_page_data(page_url, notion_api_key):\n",
    "    # Extract page ID from URL\n",
    "    page_id = page_url.split('-')[-1]\n",
    "    \n",
    "    # Set up headers with API key\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {notion_api_key}',\n",
    "        'Notion-Version': '2022-06-28',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # API endpoint for retrieving page content\n",
    "    url = f'https://api.notion.com/v1/blocks/{page_id}/children'\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Process the blocks into a more usable format\n",
    "        processed_data = []\n",
    "        for block in data.get('results', []):\n",
    "            block_type = block.get('type')\n",
    "            if block_type == 'paragraph':\n",
    "                text = ''.join([text.get('plain_text', '') for text in block[block_type].get('rich_text', [])])\n",
    "                if text:\n",
    "                    processed_data.append({\n",
    "                        'type': 'paragraph',\n",
    "                        'content': text\n",
    "                    })\n",
    "            elif block_type == 'heading_1':\n",
    "                text = ''.join([text.get('plain_text', '') for text in block[block_type].get('rich_text', [])])\n",
    "                if text:\n",
    "                    processed_data.append({\n",
    "                        'type': 'heading_1',\n",
    "                        'content': text\n",
    "                    })\n",
    "            elif block_type == 'heading_2':\n",
    "                text = ''.join([text.get('plain_text', '') for text in block[block_type].get('rich_text', [])])\n",
    "                if text:\n",
    "                    processed_data.append({\n",
    "                        'type': 'heading_2',\n",
    "                        'content': text\n",
    "                    })\n",
    "            elif block_type == 'bulleted_list_item':\n",
    "                text = ''.join([text.get('plain_text', '') for text in block[block_type].get('rich_text', [])])\n",
    "                if text:\n",
    "                    processed_data.append({\n",
    "                        'type': 'bullet_point',\n",
    "                        'content': text\n",
    "                    })\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Notion page: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_notion_content(notion_url, notion_api_key):\n",
    "    \"\"\"\n",
    "    Chat with the AI model using content from a Notion page.\n",
    "    \n",
    "    Args:\n",
    "        notion_url (str): The URL of the Notion page\n",
    "        notion_api_key (str): Your Notion API key\n",
    "    \"\"\"\n",
    "    # Get the Notion page data\n",
    "    notion_data = get_notion_page_data(notion_url, notion_api_key)\n",
    "    \n",
    "    if not notion_data:\n",
    "        print(\"Failed to fetch Notion page data.\")\n",
    "        return\n",
    "    \n",
    "    # Convert Notion data to a readable format\n",
    "    content = \"\\n\".join([f\"{item['type']}: {item['content']}\" for item in notion_data])\n",
    "    \n",
    "    # Initialize conversation\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that can reference the provided Notion content.'},\n",
    "        {'role': 'user', 'content': f'Here is the content from my Notion page:\\n\\n{content}\\n\\nPlease help me understand this content.'}\n",
    "    ]\n",
    "    \n",
    "    print('Starting conversation with Notion content...')\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "        \n",
    "        # Check if user wants to quit\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Add user message to history\n",
    "            messages.append({\n",
    "                'role': 'user',\n",
    "                'content': user_input\n",
    "            })\n",
    "            \n",
    "            # Get response from Ollama\n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            messages.append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_response\n",
    "            })\n",
    "            \n",
    "            print('\\nAssistant: ', assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Please try again or type \"quit\" to exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching Notion page: 400 Client Error: Bad Request for url: https://api.notion.com/v1/blocks/1dd9a3a6132180b6bccfc7cfa39bd39b?pvs=4/children\n"
     ]
    }
   ],
   "source": [
    "notion_url = \"https://www.notion.so/Machine-Learning-Notes-1dd9a3a6132180b6bccfc7cfa39bd39b?pvs=4\"\n",
    "notion_api_key = notion_token\n",
    "get_notion_page_data(notion_url, notion_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and process Notion page content\n",
    "def fetch_notion_page(page_id, api_key):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Notion-Version\": \"2022-06-28\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # First get the page metadata\n",
    "        page_url = f\"https://api.notion.com/v1/pages/{page_id}\"\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        page_data = response.json()\n",
    "        \n",
    "        # Then get the page content\n",
    "        blocks_url = f\"https://api.notion.com/v1/blocks/{page_id}/children\"\n",
    "        response = requests.get(blocks_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        blocks_data = response.json()\n",
    "        \n",
    "        # Process and return the content\n",
    "        content = []\n",
    "        for block in blocks_data.get('results', []):\n",
    "            if block['type'] == 'paragraph':\n",
    "                content.append(block['paragraph']['rich_text'][0]['plain_text'])\n",
    "            elif block['type'] == 'heading_1':\n",
    "                content.append(f\"# {block['heading_1']['rich_text'][0]['plain_text']}\")\n",
    "            elif block['type'] == 'heading_2':\n",
    "                content.append(f\"## {block['heading_2']['rich_text'][0]['plain_text']}\")\n",
    "            elif block['type'] == 'heading_3':\n",
    "                content.append(f\"### {block['heading_3']['rich_text'][0]['plain_text']}\")\n",
    "        \n",
    "        return '\\n'.join(content)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Notion page: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to chat with Notion content\n",
    "def chat_with_notion_content(notion_url, api_key):\n",
    "    # Extract page ID from URL\n",
    "    page_id = notion_url.split('/')[-1].split('?')[0]\n",
    "    \n",
    "    # Fetch content\n",
    "    content = fetch_notion_page(page_id, api_key)\n",
    "    if not content:\n",
    "        print(\"Failed to fetch Notion page data.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant. Here is the content from the Notion page:\\n\\n{content}\"},\n",
    "    ]\n",
    "    \n",
    "    print('Starting conversation with Notion content...')\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            messages.append({\n",
    "                'role': 'user',\n",
    "                'content': user_input\n",
    "            })\n",
    "            \n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            messages.append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_response\n",
    "            })\n",
    "            \n",
    "            print('\\nAssistant: ', assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Please try again or type \"quit\" to exit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching Notion page: 404 Client Error: Not Found for url: https://api.notion.com/v1/pages/1dd9a3a6132180b6bccfc7cfa39bd39b\n"
     ]
    }
   ],
   "source": [
    "fetch_notion_page('1dd9a3a6132180b6bccfc7cfa39bd39b', notion_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the markdown file into vector\n",
    "\n",
    "\n",
    "def convert_markdown_to_json(markdown_content):\n",
    "    \"\"\"\n",
    "    Convert markdown content to a structured JSON format\n",
    "    \"\"\"\n",
    "    # Split content into sections based on headers\n",
    "    sections = []\n",
    "    current_section = {\"title\": \"\", \"content\": []}\n",
    "    \n",
    "    for line in markdown_content.split('\\n'):\n",
    "        if line.startswith('#'):\n",
    "            # Save previous section if exists\n",
    "            if current_section[\"title\"]:\n",
    "                sections.append(current_section)\n",
    "            # Start new section\n",
    "            current_section = {\n",
    "                \"title\": line.strip('# '),\n",
    "                \"content\": []\n",
    "            }\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_section[\"content\"].append(line.strip())\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section[\"title\"]:\n",
    "        sections.append(current_section)\n",
    "    \n",
    "    return {\"sections\": sections}\n",
    "\n",
    "def ingest_content_to_model(content_json):\n",
    "    \"\"\"\n",
    "    Ingest the JSON content into the conversation context\n",
    "    \"\"\"\n",
    "    # Format the content for the model\n",
    "    formatted_content = \"Document Structure:\\n\\n\"\n",
    "    \n",
    "    for section in content_json[\"sections\"]:\n",
    "        formatted_content += f\"## {section['title']}\\n\"\n",
    "        formatted_content += \"\\n\".join(section['content']) + \"\\n\\n\"\n",
    "    \n",
    "    # Update the system message with the formatted content\n",
    "    messages[0][\"content\"] = f\"You are a helpful assistant. Here is the content from the document:\\n\\n{formatted_content}\"\n",
    "    \n",
    "    return formatted_content\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "markdown_json = convert_markdown_to_json(content)\n",
    "ingested_content = ingest_content_to_model(markdown_json)\n",
    "print(\"Content has been ingested into the model. You can now ask questions about the document.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_markdown_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a markdown file and converts it into a structured format.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the markdown file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing structured content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "        # Split content into sections based on headers\n",
    "        sections = []\n",
    "        current_section = {'type': 'paragraph', 'content': ''}\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check for headers\n",
    "            if line.startswith('# '):\n",
    "                if current_section['content']:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {'type': 'heading_1', 'content': line[2:]}\n",
    "            elif line.startswith('## '):\n",
    "                if current_section['content']:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {'type': 'heading_2', 'content': line[3:]}\n",
    "            elif line.startswith('- '):\n",
    "                if current_section['content']:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {'type': 'bullet_point', 'content': line[2:]}\n",
    "            elif line:\n",
    "                if current_section['type'] == 'paragraph':\n",
    "                    current_section['content'] += line + ' '\n",
    "                else:\n",
    "                    sections.append(current_section)\n",
    "                    current_section = {'type': 'paragraph', 'content': line + ' '}\n",
    "            elif not line and current_section['content']:\n",
    "                sections.append(current_section)\n",
    "                current_section = {'type': 'paragraph', 'content': ''}\n",
    "        \n",
    "        # Add the last section if it has content\n",
    "        if current_section['content']:\n",
    "            sections.append(current_section)\n",
    "            \n",
    "        return sections\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading markdown file: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Chat with the AI model using content from a markdown file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the markdown file\n",
    "    \"\"\"\n",
    "    # Initialize mem0ai client\n",
    "    memory_client = MemoryClient(api_key=\"m0-I2ZfUfDPh6Eq10f4pyFnxJon9TYsuQ9EycyFMi70\")\n",
    "    \n",
    "    # Read and process markdown content\n",
    "    markdown_data = read_markdown_file(file_path)\n",
    "    \n",
    "    if not markdown_data:\n",
    "        print(\"Failed to read markdown file.\")\n",
    "        return\n",
    "    \n",
    "    # Convert markdown data to a readable format\n",
    "    content = \"\\n\".join([f\"{item['type']}: {item['content']}\" for item in markdown_data])\n",
    "    \n",
    "    # Initialize conversation with markdown content\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant that can reference the provided markdown content.'},\n",
    "        {'role': 'user', 'content': f'Here is the content from my markdown file:\\n\\n{content}\\n\\nPlease help me understand this content.'}\n",
    "    ]\n",
    "    \n",
    "    print('Starting conversation with markdown content...')\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "        \n",
    "        # Check if user wants to quit\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Add user message to history\n",
    "            messages.append({\n",
    "                'role': 'user',\n",
    "                'content': user_input\n",
    "            })\n",
    "            \n",
    "            # Get response from Ollama\n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            messages.append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_response\n",
    "            })\n",
    "            \n",
    "            # Store the conversation in memory\n",
    "            memory_client.add([\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ], user_id=\"user\")\n",
    "            \n",
    "            print('\\nAssistant: ', assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Please try again or type \"quit\" to exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'heading_1', 'content': 'Feature Engineering Concepts'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '[](https://www.notion.so/1dd9a3a6132180ed89ffd9408a7098cd?pvs=21) '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_1', 'content': '01. AI vs ML vs DL vs DS'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### **1. AI (Artificial Intelligence)** '}, {'type': 'bullet_point', 'content': '**Definition:** The broadest concept — AI is creating machines or systems that can perform tasks that typically require human intelligence.'}, {'type': 'bullet_point', 'content': '**Examples:** Speech recognition (Siri, Alexa), game playing (Chess, Go), recommendation systems.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### **2. ML (Machine Learning)** '}, {'type': 'bullet_point', 'content': '**Definition:** A **subset of AI**. ML refers to systems that can **learn from data** and **improve over time** without being explicitly programmed for every rule.'}, {'type': 'bullet_point', 'content': '**Examples:** Spam email detection, image recognition, and fraud detection.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### **3. DL (Deep Learning)** '}, {'type': 'bullet_point', 'content': '**Definition:** A **subset of ML**. It uses **neural networks with many layers** (hence “deep”) to model complex patterns in large data.'}, {'type': 'bullet_point', 'content': '**Examples:** Self-driving cars (vision systems), voice assistants, and facial recognition.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### **4. DS (Data Science)** '}, {'type': 'bullet_point', 'content': '**Definition:** A broader discipline that combines **statistics, data analysis, and domain expertise** to extract insights from data. Data Science often uses ML and DL as tools.'}, {'type': 'bullet_point', 'content': '**Examples:** Business intelligence, data-driven decision making, predictive analytics.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### **In Venn Diagram Form:** '}, {'type': 'paragraph', 'content': '``` +--------------------+ |      AI            | |  +-------------+   | |  |   ML        |   | |  |  +-------+  |   | |  |  |  DL   |  |   | |  |  +-------+  |   | |  +-------------+   | +--------------------+ (DS overlaps with all — acts like an umbrella toolset) '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### Real World analogy: '}, {'type': 'bullet_point', 'content': '**AI** = Goal (simulate intelligence)'}, {'type': 'bullet_point', 'content': '**ML** = Approach (learn from data)'}, {'type': 'bullet_point', 'content': '**DL** = Technique (neural networks)'}, {'type': 'bullet_point', 'content': '**DS** = Practice (extract insights from data using stats/ML)'}, {'type': 'paragraph', 'content': 'Want a fun analogy or real-world example to help it stick? '}, {'type': 'heading_1', 'content': '02. Supervised, Unsupervised, and Reinforcement learning'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '**1. Supervised Learning**'}, {'type': 'paragraph', 'content': '> \"Learn from labeled examples.\" > '}, {'type': 'paragraph', 'content': '### 🔹 How it works: '}, {'type': 'bullet_point', 'content': 'The algorithm is trained on a **labeled dataset**, meaning each input has a correct output.'}, {'type': 'bullet_point', 'content': 'The model learns to map inputs to outputs.'}, {'type': 'paragraph', 'content': '### 🧠 Think of it like: '}, {'type': 'paragraph', 'content': 'A student learning with an answer key. '}, {'type': 'paragraph', 'content': '### 📦 Examples: '}, {'type': 'bullet_point', 'content': '**Spam detection** (email → spam or not spam)'}, {'type': 'bullet_point', 'content': '**Predicting house prices** (input: size, location → output: price)'}, {'type': 'bullet_point', 'content': '**Image classification** (picture of animal → label: cat/dog/etc)'}, {'type': 'paragraph', 'content': '### 📊 Types: '}, {'type': 'bullet_point', 'content': '**Regression** (predict continuous values)'}, {'type': 'bullet_point', 'content': '**Classification** (predict categories)'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '**2. Unsupervised Learning**'}, {'type': 'paragraph', 'content': '> \"Find hidden patterns in unlabeled data\" > '}, {'type': 'paragraph', 'content': '### 🔹 How it works: '}, {'type': 'bullet_point', 'content': 'No labeled outputs; the model finds **structure** or **patterns** in the data.'}, {'type': 'paragraph', 'content': '### 🧠 Think of it like: '}, {'type': 'paragraph', 'content': 'An explorer groups similar things without knowing what they are. '}, {'type': 'paragraph', 'content': '### 📦 Examples: '}, {'type': 'bullet_point', 'content': '**Customer segmentation** (grouping customers by behavior)'}, {'type': 'bullet_point', 'content': '**Anomaly detection** (fraud detection, manufacturing defects)'}, {'type': 'bullet_point', 'content': '**Topic modeling** (finding topics in large documents)'}, {'type': 'paragraph', 'content': '### 📊 Types: '}, {'type': 'bullet_point', 'content': '**Clustering** (e.g., K-means)'}, {'type': 'bullet_point', 'content': '**Dimensionality reduction** (e.g., PCA, t-SNE)'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '**3. Reinforcement Learning**'}, {'type': 'paragraph', 'content': '> \"Learn by trial and error with rewards.\" > '}, {'type': 'paragraph', 'content': '### 🔹 How it works: '}, {'type': 'bullet_point', 'content': 'An **agent** interacts with an **environment** and learns to make decisions by receiving **rewards** or **penalties**.'}, {'type': 'bullet_point', 'content': 'Goal: maximize total reward over time.'}, {'type': 'paragraph', 'content': '### 🧠 Think of it like: '}, {'type': 'paragraph', 'content': 'Training a dog with treats — good actions get rewards, bad actions don’t. '}, {'type': 'paragraph', 'content': '### 📦 Examples: '}, {'type': 'bullet_point', 'content': '**Game playing** (like AlphaGo, Dota bots)'}, {'type': 'bullet_point', 'content': '**Robotics** (learning to walk, grip)'}, {'type': 'bullet_point', 'content': '**Self-driving cars** (learning to navigate safely)'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### Table: '}, {'type': 'paragraph', 'content': '| Type | Data Used | Goal | Example | | --- | --- | --- | --- | | **Supervised** | Labeled data | Predict known outcomes | Email spam detection | | **Unsupervised** | Unlabeled data | Find patterns or groupings. | Customer segmentation | | **Reinforcement** | Trial & error | Learn optimal actions over time. | AI playing chess or driving cars. | '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': 'Want a visual analogy or mini real-world story to help you remember these? '}, {'type': 'heading_1', 'content': '03. Train, Test, and Validation'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🔧 **1. Training Set**'}, {'type': 'paragraph', 'content': '> \"Where learning happens.\" > '}, {'type': 'bullet_point', 'content': 'This is the **data the model learns from**.'}, {'type': 'bullet_point', 'content': 'It contains both inputs and correct outputs (in supervised learning).'}, {'type': 'bullet_point', 'content': 'The model adjusts its internal parameters (like weights in a neural network) to minimize error on this set.'}, {'type': 'paragraph', 'content': '### 🔁 Think of it like: '}, {'type': 'paragraph', 'content': 'Studying from a textbook. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🔍 **2. Validation Set**'}, {'type': 'paragraph', 'content': '> \"Where tuning happens.\" > '}, {'type': 'bullet_point', 'content': 'Used to **tune hyperparameters** (like learning rate, number of layers, etc.).'}, {'type': 'bullet_point', 'content': 'Helps **prevent overfitting** by testing the model on unseen (but still known) data during training.'}, {'type': 'bullet_point', 'content': 'The model doesn’t learn from this set — it’s only evaluated on it.'}, {'type': 'paragraph', 'content': '### 🧪 Think of it like: '}, {'type': 'paragraph', 'content': 'Taking practice tests while studying to see what works best. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🧾 **3. Test Set**'}, {'type': 'paragraph', 'content': '> \"Where final evaluation happens.\" > '}, {'type': 'bullet_point', 'content': 'Completely **held-out data** — the model has never seen it before.'}, {'type': 'bullet_point', 'content': 'Used **only once** after training and tuning, to see how well the model generalizes to **new, real-world data**.'}, {'type': 'bullet_point', 'content': 'It should be untouched during the training process.'}, {'type': 'paragraph', 'content': '### 🎓 Think of it like: '}, {'type': 'paragraph', 'content': 'Your final exam — no more learning, just show what you know. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🧠 Why this 3-way split?'}, {'type': 'bullet_point', 'content': '**Training set**: to **learn** patterns'}, {'type': 'bullet_point', 'content': '**Validation set**: to **tune** the model and **avoid overfitting**'}, {'type': 'bullet_point', 'content': '**Test set**: to **evaluate** real-world performance'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '📊 Typical Splits:'}, {'type': 'paragraph', 'content': '| Dataset | % of Total Data | | --- | --- | | Training | 60–70% | | Validation | 10–20% | | Test | 20% | '}, {'type': 'paragraph', 'content': '*(Sometimes people use just train/test and do cross-validation instead of a separate validation set.)* '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🛠️ Bonus: Cross-Validation (CV)'}, {'type': 'paragraph', 'content': 'Instead of having a fixed validation set, you: '}, {'type': 'bullet_point', 'content': 'Split the data into *k* parts (e.g., 5)'}, {'type': 'bullet_point', 'content': 'Train on 4 parts, validate on 1 — repeat 5 times.'}, {'type': 'bullet_point', 'content': 'Average the results'}, {'type': 'paragraph', 'content': '→ Great when you have limited data! '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': 'Let me know if you want a simple diagram to visualize this or a real-life analogy to lock it in! '}, {'type': 'heading_1', 'content': '04. Bias, Variance, Overfitting, and Underfitting'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🔁 Bias vs. Variance'}, {'type': 'paragraph', 'content': '### 🔷 **Bias** '}, {'type': 'paragraph', 'content': '> Error from wrong assumptions in the learning algorithm. > '}, {'type': 'bullet_point', 'content': 'A high-bias model is **too simple**, missing patterns in the data.'}, {'type': 'bullet_point', 'content': \"It doesn't learn enough — **underfits**.\"}, {'type': 'paragraph', 'content': '📉 **High bias = Low performance on both training & test data** '}, {'type': 'paragraph', 'content': '**Examples:** '}, {'type': 'bullet_point', 'content': 'Using a linear model to fit complex, non-linear data'}, {'type': 'bullet_point', 'content': 'Assuming all emails with “free” are spam (too general)'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔶 **Variance** '}, {'type': 'paragraph', 'content': '> Error from model sensitivity to small fluctuations in training data. > '}, {'type': 'bullet_point', 'content': 'A high-variance model is **too complex**, learns noise as if it were signal.'}, {'type': 'bullet_point', 'content': 'It memorizes the data — **overfits**.'}, {'type': 'paragraph', 'content': '📉 **High variance = Great on training, bad on test data** '}, {'type': 'paragraph', 'content': '**Examples:** '}, {'type': 'bullet_point', 'content': 'Very deep decision trees'}, {'type': 'bullet_point', 'content': 'Neural networks with no regularization'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🎭 Overfitting vs. Underfitting'}, {'type': 'paragraph', 'content': '| Concept | Description | Cause | Looks Like | | --- | --- | --- | --- | | **Overfitting** | The model fits the training data **too well**, including noise. | **High variance** | Low train error, high test error | | **Underfitting** | The model can’t capture the underlying pattern. | **High bias** | High error on both sets | '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 📈 Visualization (Mental Image) '}, {'type': 'paragraph', 'content': 'Imagine fitting curves to a scatter plot: '}, {'type': 'bullet_point', 'content': '**Underfitting**: Just a straight line — misses all the curves.'}, {'type': 'bullet_point', 'content': '**Good Fit**: A nice smooth curve that follows the trend.'}, {'type': 'bullet_point', 'content': '**Overfitting**: A crazy zig-zag line — hits every point exactly, even outliers.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '⚖️ The Tradeoff'}, {'type': 'paragraph', 'content': 'You want to balance bias and variance to **minimize total error**: '}, {'type': 'paragraph', 'content': '📉 **Total Error = Bias² + Variance + Irreducible Error** '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🛠️ Solutions:'}, {'type': 'paragraph', 'content': '| Problem | Fixes | | --- | --- | | **Underfitting** | Use a more complex model, and add features. | | **Overfitting** | Use regularization, simplify the model, use more data, and cross-validation. | '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': 'TL;DR'}, {'type': 'bullet_point', 'content': '**Bias** = too simple → underfit'}, {'type': 'bullet_point', 'content': '**Variance** = too complex → overfit'}, {'type': 'bullet_point', 'content': '**Good model** = low bias, low variance'}, {'type': 'bullet_point', 'content': 'The goal: just the right complexity for your data 🍌'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': 'Want a goofy analogy or a cartoon-style sketch of this? Happy to make it fun! '}, {'type': 'heading_1', 'content': '05. Handling Missing Values'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '🔍 First: Identify Missing Values'}, {'type': 'paragraph', 'content': 'Check for: '}, {'type': 'bullet_point', 'content': '`NaN` (Not a Number)'}, {'type': 'bullet_point', 'content': 'Blank cells'}, {'type': 'bullet_point', 'content': \"Placeholders like `'?'`, `'NA'`, `'None'`, or `999`\"}, {'type': 'paragraph', 'content': 'In **Python/pandas**: '}, {'type': 'paragraph', 'content': '```python df.isnull().sum() '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '✅ Common Techniques to Handle Missing Data'}, {'type': 'paragraph', 'content': '### 1. **Remove Data** '}, {'type': 'paragraph', 'content': '### 🔹 a. Drop Rows '}, {'type': 'bullet_point', 'content': 'If only a few rows are missing values.'}, {'type': 'paragraph', 'content': '```python df.dropna(inplace=True) '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '### 🔹 b. Drop Columns '}, {'type': 'bullet_point', 'content': 'If a column has **too many missing values** (e.g.,>50%).'}, {'type': 'paragraph', 'content': \"```python df.drop(columns=['column_name'], inplace=True) \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 2. **Impute (Fill In) Missing Values** '}, {'type': 'paragraph', 'content': '> Better than dropping — retains data and structure. > '}, {'type': 'paragraph', 'content': '### 🔹 a. **Mean / Median / Mode** '}, {'type': 'bullet_point', 'content': '**Numerical features**: use **mean** or **median**'}, {'type': 'bullet_point', 'content': '**Categorical features**: use **mode**'}, {'type': 'paragraph', 'content': \"```python df['age'].fillna(df['age'].mean(), inplace=True) df['gender'].fillna(df['gender'].mode()[0], inplace=True) \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '### 🔹 b. **Forward Fill / Backward Fill** '}, {'type': 'bullet_point', 'content': 'Use the previous or next value in the column (good for time series)'}, {'type': 'paragraph', 'content': \"```python df.fillna(method='ffill', inplace=True)  # Forward fill df.fillna(method='bfill', inplace=True)  # Backward fill \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '### 🔹 c. **Predictive Imputation** '}, {'type': 'bullet_point', 'content': 'Use a **machine learning model** (like KNN or regression) to predict missing values based on other features.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 3. **Flag Missingness** '}, {'type': 'bullet_point', 'content': 'Add a new binary column indicating where values were missing.'}, {'type': 'paragraph', 'content': \"```python df['age_missing'] = df['age'].isnull().astype(int) \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': 'Useful if \"missingness\" itself carries information (e.g., missing income could signal something meaningful). '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 4. **Use Models That Handle Missing Data** '}, {'type': 'bullet_point', 'content': 'Some algorithms can **natively handle missing values**, like:'}, {'type': 'bullet_point', 'content': 'XGBoost'}, {'type': 'bullet_point', 'content': 'LightGBM'}, {'type': 'bullet_point', 'content': 'CatBoost'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '⚠️ Things to Watch Out For'}, {'type': 'bullet_point', 'content': '**Don’t leak test data into imputation** (do it only with training data).'}, {'type': 'bullet_point', 'content': 'Always compare performance **before and after** imputation.'}, {'type': 'bullet_point', 'content': 'Use **domain knowledge** when possible (e.g., fill “Blood Type” with “Unknown”, not the mode).'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': 'TL;DR Table:'}, {'type': 'paragraph', 'content': '| Method | Best for | Risk | | --- | --- | --- | | Drop rows/columns | Sparse missing data | Lose data | | Mean/Median/Mode | Simple numeric/categorical | May bias distribution | | Ffill/Bfill | Time series data | Assumes trend continuity | | Predictive impute | Complex, structured datasets | Time-consuming | | Missing flags | When missing is informative. | Adds new features | '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_1', 'content': '06. Handling Imbalanced Datasets'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'heading_2', 'content': '6.1 Upscaling and Downscaling:'}, {'type': 'paragraph', 'content': '### ⚖️ **Imbalanced Datasets** '}, {'type': 'bullet_point', 'content': 'A dataset where one class (the **majority**) has **many more samples** than the other class (the **minority**).'}, {'type': 'bullet_point', 'content': 'Common in areas like fraud detection, medical diagnosis, and churn prediction.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔁 **Rescaling in Imbalanced Datasets** '}, {'type': 'paragraph', 'content': 'In this context, **rescaling** often refers to **resampling**, which includes: '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔼 **Upscaling / Oversampling** '}, {'type': 'bullet_point', 'content': '**Increases** the number of samples in the **minority class**.'}, {'type': 'bullet_point', 'content': 'Goal: balance the class distribution by **adding more minority samples**.'}, {'type': 'paragraph', 'content': '### 🧪 Techniques: '}, {'type': 'paragraph', 'content': '1. **Random Oversampling** '}, {'type': 'bullet_point', 'content': 'Randomly duplicates samples from the minority class.'}, {'type': 'bullet_point', 'content': 'Simple but may cause **overfitting**.'}, {'type': 'paragraph', 'content': '2. **SMOTE (Synthetic Minority Over-sampling Technique)** '}, {'type': 'bullet_point', 'content': 'Generates **synthetic samples** based on existing ones using interpolation.'}, {'type': 'bullet_point', 'content': 'Helps avoid overfitting compared to random oversampling.'}, {'type': 'paragraph', 'content': '3. **ADASYN** '}, {'type': 'bullet_point', 'content': 'Like SMOTE, but focuses more on **harder-to-learn** samples.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔽 **Downscaling / Undersampling** '}, {'type': 'bullet_point', 'content': '**Reduces** the number of samples in the **majority class**.'}, {'type': 'bullet_point', 'content': 'Goal: balance the dataset by **removing some majority samples**.'}, {'type': 'paragraph', 'content': '### 🧪 Techniques: '}, {'type': 'paragraph', 'content': '1. **Random Undersampling** '}, {'type': 'bullet_point', 'content': 'Randomly drops samples from the majority class.'}, {'type': 'bullet_point', 'content': 'Fast and easy, but may **lose important data**.'}, {'type': 'paragraph', 'content': '2. **Tomek Links / Edited Nearest Neighbors** '}, {'type': 'bullet_point', 'content': 'Removes ambiguous or borderline majority samples.'}, {'type': 'bullet_point', 'content': 'Aims to clean class boundaries rather than just drop samples.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### ⚖️ **When to Use What?** '}, {'type': 'paragraph', 'content': '| Scenario | Technique | Pros | Cons | | --- | --- | --- | --- | | Small dataset | **Oversampling** | Keeps all data, adds more | Risk of overfitting | | Large dataset | **Undersampling** | Reduces training time | Risk of information loss | | Complex boundary | **SMOTE / ADASYN** | Creates balanced, diverse data | Needs careful tuning | '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🛠️ Python Example (imbalanced-learn) '}, {'type': 'paragraph', 'content': '```python from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler '}, {'type': 'heading_1', 'content': 'Upsampling'}, {'type': 'paragraph', 'content': 'X_up, y_up = SMOTE().fit_resample(X, y) '}, {'type': 'heading_1', 'content': 'Downsampling'}, {'type': 'paragraph', 'content': 'X_down, y_down = RandomUnderSampler().fit_resample(X, y) '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🧪 Example with `resample` (from `sklearn`) '}, {'type': 'paragraph', 'content': '### 📚 Setup: Simulate an Imbalanced Dataset '}, {'type': 'paragraph', 'content': '```python import numpy as np, import pandas as pd, from sklearn.utils import resample '}, {'type': 'heading_1', 'content': 'Create a simple imbalanced dataset'}, {'type': 'paragraph', 'content': \"data = { 'feature': np.random.randn(100), 'target': [0]*90 + [1]*10  # Class 0 = majority, Class 1 = minority } \"}, {'type': 'paragraph', 'content': 'df = pd.DataFrame(data) '}, {'type': 'heading_1', 'content': 'Split majority and minority'}, {'type': 'paragraph', 'content': \"df_majority = df[df['target'] == 0] df_minority = df[df['target'] == 1] \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔼 Upsampling (Minority Class) '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'Upsample minority class'}, {'type': 'paragraph', 'content': 'df_minority_upsampled = resample( df_minority, replace=True,       # sample with replacement n_samples=90,       # match the number of the majority class random_state=42     # reproducibility ) '}, {'type': 'heading_1', 'content': 'Combine majority and upsampled minority'}, {'type': 'paragraph', 'content': 'df_upsampled = pd.concat([df_majority, df_minority_upsampled]) '}, {'type': 'paragraph', 'content': \"print(df_upsampled['target'].value_counts()) \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '✅ Now both classes have **90 samples** each — balanced. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🔽 Downsampling (Majority Class) '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'Downsample the majority class'}, {'type': 'paragraph', 'content': 'df_majority_downsampled = resample( df_majority, replace=False,      # sample without replacement n_samples=10,       # match the number of minority class random_state=42 ) '}, {'type': 'heading_1', 'content': 'Combine downsampled majority and minority'}, {'type': 'paragraph', 'content': 'df_downsampled = pd.concat([df_majority_downsampled, df_minority]) '}, {'type': 'paragraph', 'content': \"print(df_downsampled['target'].value_counts()) \"}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '✅ Now both classes have **10 samples** each — balanced but with less data. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### ⚠️ Notes '}, {'type': 'bullet_point', 'content': 'Use **upsampling** when you want to preserve all data and add synthetic or duplicate samples.'}, {'type': 'bullet_point', 'content': \"Use **downsampling** when your dataset is large and you're okay with losing some data for balance.\"}, {'type': 'paragraph', 'content': 'Let me know if you want this adapted for a real dataset like `sklearn.datasets.make_classification` or with SMOTE! '}, {'type': 'heading_2', 'content': '6.2 SMOTE(**Synthetic Minority Over-sampling Technique)**'}, {'type': 'paragraph', 'content': '[https://github.com/anuj-kumar-30/ML_pw.git](https://github.com/anuj-kumar-30/ML_pw.git) '}, {'type': 'paragraph', 'content': '[https://github.com/anuj-kumar-30/ML_pw/tree/7b8dbe17dca1b698612d073208849ae1ef6e6d14/ML19_ML_Concepts](https://github.com/anuj-kumar-30/ML_pw/tree/7b8dbe17dca1b698612d073208849ae1ef6e6d14/ML19_ML_Concepts) '}, {'type': 'paragraph', 'content': '### 📌 **What is SMOTE?** '}, {'type': 'bullet_point', 'content': '**SMOTE** is a technique used to handle **class imbalance** in datasets, particularly in classification tasks.'}, {'type': 'bullet_point', 'content': 'It **generates synthetic samples** of the minority class to balance the dataset, instead of simply duplicating existing samples.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### ⚙️ **How Does SMOTE Work?** '}, {'type': 'paragraph', 'content': '1. **Identify minority class samples**. 2. For each sample: '}, {'type': 'bullet_point', 'content': 'Find its **k-nearest neighbors** (usually `k=5`) within the minority class.'}, {'type': 'paragraph', 'content': '3. Randomly select one or more of these neighbors. 4. Create a synthetic sample: '}, {'type': 'bullet_point', 'content': 'Choose a point on the line between the original sample and the neighbor:'}, {'type': 'paragraph', 'content': 'Synthetic\\xa0point=x+δ⋅(xneighbor−x)\\\\text{Synthetic point} = x + \\\\delta \\\\cdot (x_{\\\\text{neighbor}} - x) '}, {'type': 'paragraph', 'content': 'where δ\\\\delta is a random number in [0,1][0, 1]. '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### ✅ **Advantages** '}, {'type': 'bullet_point', 'content': 'Reduces overfitting compared to simple oversampling (replication).'}, {'type': 'bullet_point', 'content': 'Improves classifier performance for imbalanced datasets.'}, {'type': 'bullet_point', 'content': 'Can be used with various classifiers (SVM, Random Forest, etc.).'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### ⚠️ **Disadvantages / Limitations** '}, {'type': 'bullet_point', 'content': 'May create **ambiguous samples** near class boundaries.'}, {'type': 'bullet_point', 'content': 'It can increase the chance of **overlapping** between classes.'}, {'type': 'bullet_point', 'content': 'Doesn’t take into account the **majority class distribution**, which can be problematic.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 🛠️ **Variants of SMOTE** '}, {'type': 'bullet_point', 'content': '**Borderline-SMOTE**: Focuses on samples near the decision boundary.'}, {'type': 'bullet_point', 'content': '**SMOTE-NC**: Handles mixed data types (numerical + categorical).'}, {'type': 'bullet_point', 'content': '**ADASYN**: Generates more synthetic data in regions where the minority class is harder to learn.'}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': '### 📚 **Use in Python (with imbalanced-learn)** '}, {'type': 'paragraph', 'content': '```python From imblearn.over_sampling import SMOTE '}, {'type': 'paragraph', 'content': 'sm = SMOTE(random_state=42) X_resampled, y_resampled = sm.fit_resample(X, y) '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '--- '}, {'type': 'paragraph', 'content': \"Let me know if you'd like diagrams, examples, or comparisons with other techniques like undersampling or ensemble methods. \"}, {'type': 'heading_2', 'content': '6.3 Data Interpolation'}, {'type': 'paragraph', 'content': '> It is the process of estimating unknown values within a dataset based on the known values. In Python, there are various libraries available that can be used for data interpolation, such as numpy, scipy, and pandas**.** > '}, {'type': 'paragraph', 'content': '```mermaid graph TD Types --> Linear-Interpolation Types --> Cubic-Interpolation Types --> Polynomial-Interpolation ``` '}, {'type': 'paragraph', 'content': '### 🔧 What is Linear Interpolation in ML? '}, {'type': 'paragraph', 'content': 'In machine learning, **linear interpolation** is used to estimate or predict intermediate values between two known data points, assuming a linear relationship between them. It’s not a learning algorithm per se, but it’s often used in: '}, {'type': 'paragraph', 'content': '1. **Data preprocessing** 2. **Model interpretability** 3. **Feature engineering** 4. **Imputation (filling in missing values)** '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'import libraries'}, {'type': 'paragraph', 'content': 'import numpy as np import matplotlib.pyplot as plt import pandas as pd '}, {'type': 'heading_1', 'content': 'Creating linear sample data where x=2y'}, {'type': 'paragraph', 'content': 'x = np.array([1,2,3,4,5]) y = np.array([2,4,6,8,10]) '}, {'type': 'heading_1', 'content': 'Plot a graph that is going to be a linear line'}, {'type': 'paragraph', 'content': 'plt.scatter(x, y) '}, {'type': 'heading_1', 'content': 'Now we will apply Linear interpolation to increase our value counts'}, {'type': 'paragraph', 'content': 'x_new = np.linspace(1,5,10) # creating new x values y_interp = np.interp(x_new, x, y) # generating new y values based on the relationship between x and y '}, {'type': 'heading_1', 'content': 'Plot a graph which also going to be same as the previous one'}, {'type': 'paragraph', 'content': 'plt.scatter(x_new, y_interp) ``` '}, {'type': 'paragraph', 'content': '### 🔍 What is Cubic Interpolation? '}, {'type': 'paragraph', 'content': 'While **linear interpolation** connects two points with a straight line, **cubic interpolation** fits a **smooth curve** (a **cubic polynomial**) through several points. This curve is defined by an equation of the form: '}, {'type': 'paragraph', 'content': 'y = ax^3 + bx^2 + cx + d '}, {'type': 'paragraph', 'content': 'It uses **4 known data points** to estimate unknown values, which allows for **smoother transitions** and better modeling of curved data trends, especially useful when the data isn’t linear. '}, {'type': 'paragraph', 'content': '```python x = np.array([1,2,3,4,5]) y = np.array([1,8,27,64,125]) plt.scatter(x,y) '}, {'type': 'paragraph', 'content': 'from scipy.interpolate import interp1d '}, {'type': 'heading_1', 'content': 'Create a cubic interpolation function'}, {'type': 'paragraph', 'content': \"f=interp1d(x,y,kind='cubic') \"}, {'type': 'heading_1', 'content': 'Interpolate the data'}, {'type': 'paragraph', 'content': 'x_new = np.linspace(1,5,10) y_interp = f(x_new) y_interp '}, {'type': 'paragraph', 'content': 'plt.scatter(x_new, y_interp) ``` '}, {'type': 'heading_2', 'content': '🔍 What Is Polynomial Interpolation?'}, {'type': 'paragraph', 'content': '**Polynomial interpolation** is the process of estimating a function that passes through a given set of data points using a **single polynomial of degree n**. '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'create some sample data'}, {'type': 'paragraph', 'content': 'x = np.array([1,2,3,4,5]) y = np.array([1,4,9,16,25]) '}, {'type': 'heading_1', 'content': 'Interpolate the data using polynomial interpolation'}, {'type': 'paragraph', 'content': 'p = np.polyfit(x, y, 2) # interpolate y values '}, {'type': 'paragraph', 'content': 'x_new = np.linspace(1,5,15) y_interp = np.polyval(p, x_new) '}, {'type': 'paragraph', 'content': 'plt.scatter(x,y) '}, {'type': 'paragraph', 'content': 'plt.scatter(x_new, y_interp) ``` '}, {'type': 'paragraph', 'content': '> In simple Outliers are those data points that are completely different or out of range from the majority of the dataset are called outliers > '}, {'type': 'paragraph', 'content': 'Why do we need to take care of these outliers? '}, {'type': 'paragraph', 'content': '> Having outliers in our datasets can drastically affect our model during the training process. > '}, {'type': 'heading_2', 'content': '7.2 How to Handle Outliers'}, {'type': 'bullet_point', 'content': '1st, we need to understand the 5 numbers summary:'}, {'type': 'paragraph', 'content': '1. Min value 2. Q1 - 25 percentile 3. Median 4. Q3-75 percentile 5. Max value '}, {'type': 'bullet_point', 'content': 'Syntax:'}, {'type': 'bullet_point', 'content': '`np.percentile(list/series, percentile)`'}, {'type': 'paragraph', 'content': '```python import numpy as np marks = [20,50,55,23,26,258,111000,100] '}, {'type': 'heading_1', 'content': 'finding 5 number value in marks'}, {'type': 'heading_1', 'content': 'syntax: np.percentile(list/series, percentile)'}, {'type': 'paragraph', 'content': \"print(f'Min_value: {np.percentile(marks, 0)}\\\\nQ1: {np.percentile(marks, 25)}\\\\nQ2: {np.percentile(marks, 50)}\\\\nQ3: {np.percentile(marks, 75)}\\\\nMax_value: {np.percentile(marks, 100)}') ``` \"}, {'type': 'paragraph', 'content': '### Method: 01(Using formulas) '}, {'type': 'bullet_point', 'content': 'To find the outliers, we will create a range [lower_fence, higher_fence] if the element is outside this range is called an outlier.'}, {'type': 'bullet_point', 'content': 'To calculate lower_fence and higher_fence, 1st we will find the quartile value for the series.'}, {'type': 'paragraph', 'content': '1. `np.quantile(lst_marks, [0.0,0.25,0.50,0.75,1.0])` 2. IQR(Inter Quartile Range) = `Q3 -Q1` 3. lower_fence = `Q1 - 1.5*IQR` 4. Higher_fence = `Q3 + 1.5*IQR` '}, {'type': 'bullet_point', 'content': 'Now anything outside this range are the outliers in the list or series.'}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'finding all 5 quartile'}, {'type': 'paragraph', 'content': 'mn, q1, q2, q3, mx = np.quantile(marks, [0.0,0.25,0.50,0.75,1.0]) mn, q1, q2, q3, mx '}, {'type': 'heading_1', 'content': 'calculating lower fence and higher fence'}, {'type': 'paragraph', 'content': 'IQR = q3-q1 lf = q1 - 1.5*IQR hf = q3 + 1.5*IQR '}, {'type': 'heading_1', 'content': 'check if number is a outlier or not'}, {'type': 'paragraph', 'content': \"for mark in marks: if (mark<lf or mark>hf): print(f'{mark} this is a outlier') ``` \"}, {'type': 'paragraph', 'content': '### Method: 02(Using Box Plot) '}, {'type': 'bullet_point', 'content': '1st create box plot using sns. `sns.boxplot(marks)`'}, {'type': 'bullet_point', 'content': 'Now anything outside this box is an outliers.'}, {'type': 'paragraph', 'content': '```python import seaborn as sns sns.boxplot(marks) ``` '}, {'type': 'heading_1', 'content': '08. Feature Extraction'}, {'type': 'paragraph', 'content': '> It is the process of selecting and extracting the most important features from raw data > '}, {'type': 'heading_2', 'content': '8.1 Feature Scaling'}, {'type': 'paragraph', 'content': '> It is the process of normalizing or standardizing the range of independent variable(features) in our datasets > '}, {'type': 'paragraph', 'content': '> This is important because many machine learning algorithms perform **better or converge faster** when features are on a **similar scale**. > '}, {'type': 'paragraph', 'content': '### 8.1.1 Why do we need Feature scaling '}, {'type': 'bullet_point', 'content': 'Many ML models are sensitive to **feature magnitudes**, such as:'}, {'type': 'paragraph', 'content': '| Algorithm | Needs Scaling? | Why? | | --- | --- | --- | | Linear Regression | ✅ | Assumes equally weighted features | | Logistic Regression | ✅ | Gradient descent convergence | | SVM | ✅ | Distance-based (dot product) | | KNN | ✅ | Based on Euclidean distance | | Neural Networks | ✅ | Faster gradient descent | | Tree-based (e.g., Random Forest, XGBoost) | ❌ | Not sensitive to feature scale | '}, {'type': 'paragraph', 'content': '### 8.1.2 Common Feature Scaling Methods '}, {'type': 'paragraph', 'content': '1. Min-Max Scaling(Normalization) '}, {'type': 'bullet_point', 'content': 'Scales features to [0, 1]'}, {'type': 'bullet_point', 'content': 'Good for: image pixel values, neural nets'}, {'type': 'paragraph', 'content': '```python from sklearn.preprocessing import MinMaxScaler '}, {'type': 'paragraph', 'content': 'scaler = MinMaxScaler() scaled = scaler.fit_transform(X) ``` '}, {'type': 'paragraph', 'content': '$$ x_{scaled} = \\\\frac{x - x_{min}}{x_{max} - x_{min}} '}, {'type': 'paragraph', 'content': '$$ '}, {'type': 'paragraph', 'content': 'b. Standardization (Z-score Normalization) '}, {'type': 'bullet_point', 'content': 'Centers data around 0 with unit variance'}, {'type': 'bullet_point', 'content': 'Good for: algorithms assuming Gaussian distribution'}, {'type': 'paragraph', 'content': '```python from sklearn.preprocessing import StandardScaler '}, {'type': 'paragraph', 'content': 'scaler = StandardScaler() scaled = scaler.fit_transform(X) ``` '}, {'type': 'paragraph', 'content': '$$ z = \\\\frac{x - \\\\mu}{\\\\sigma} $$ '}, {'type': 'paragraph', 'content': 'c. Robust Scaling '}, {'type': 'bullet_point', 'content': 'Uses median and interquartile range'}, {'type': 'bullet_point', 'content': 'Good for: datasets with outliers'}, {'type': 'paragraph', 'content': '```python from sklearn.preprocessing import RobustScaler '}, {'type': 'paragraph', 'content': 'scaler = RobustScaler() scaled = scaler.fit_transform(X) ``` '}, {'type': 'paragraph', 'content': '$$ \\\\frac{X - X_{median}}{IQR} $$ '}, {'type': 'paragraph', 'content': 'd. MaxAbs Scaling '}, {'type': 'bullet_point', 'content': 'Scales data to [-1, 1] based on absolute max.'}, {'type': 'bullet_point', 'content': 'Useful for sparse data(eg: TF-IDF vectors)'}, {'type': 'paragraph', 'content': \"$$ X' = \\\\frac{X}{|{X_{max}}|} $$ \"}, {'type': 'paragraph', 'content': '### Summary '}, {'type': 'paragraph', 'content': '| Method | Use When... | Sensitive to Outliers? | | --- | --- | --- | | Min-Max | Bounded input needed (e.g., images) | ✅ Yes | | Standardization | Features have different units | ✅ Yes | | Robust | Data has many outliers | ❌ No | | MaxAbs | Sparse data (e.g., NLP) | ✅ Yes | '}, {'type': 'heading_2', 'content': '8.2 Feature Selection'}, {'type': 'paragraph', 'content': '> Here we just pick the most important features. > '}, {'type': 'paragraph', 'content': '### 8.2.1 How do we filter the features: '}, {'type': 'paragraph', 'content': '1. Filter Method 2. Embedded Method '}, {'type': 'bullet_point', 'content': 'PCA'}, {'type': 'paragraph', 'content': '🧠 What is PCA? '}, {'type': 'paragraph', 'content': '> It is a dimensionality reduction technique. It transforms a dataset with many possibly correlated features into a smaller number of uncorrelated features called principle components, while preserving as much variance as possible. > '}, {'type': 'paragraph', 'content': '✅ Why Use PCA? '}, {'type': 'paragraph', 'content': '> > > > > | Reason | Benefit | > | --- | --- | > | 🚀 Speed up ML models | Reduces feature space, fewer computations | > | 🧹 Reduce noise | Removes less important information | > | 📉 Prevent overfitting | Lower dimensional space = fewer chances to overfit | > | 📊 Better visualization | Makes high-dimensional data plottable (e.g., in 2D/3D) | '}, {'type': 'paragraph', 'content': '📐 How Does PCA Work? '}, {'type': 'paragraph', 'content': '> > > 1. Standardize the data. > 2. Compute Covariance Matrix: This shows how features vary together > 3. Calculate Eigenvectors and Eigenvalues. >     1. Eigenvectors = new feature directions >     2. Eigenvalues = how much variance each direction explains > 4. Sort and Select Components.: Choose top k components that explain most variance. > 5. Transform Data: Project the data onto those k principle components. '}, {'type': 'heading_2', 'content': '✅ When to Use PCA'}, {'type': 'bullet_point', 'content': \"Too many features (especially if they're correlated)\"}, {'type': 'bullet_point', 'content': 'You want faster training or visualization'}, {'type': 'bullet_point', 'content': 'You want to reduce noise or overfitting'}, {'type': 'heading_1', 'content': '09. Data Encoding'}, {'type': 'paragraph', 'content': '> AIM —> Categorical features —> Numerical —> ML model —> Train > '}, {'type': 'heading_2', 'content': '9.1 Types of Data encoding'}, {'type': 'bullet_point', 'content': 'Nominal/ OHE(One Hot Encoding)'}, {'type': 'paragraph', 'content': 'Q. What is Nominal Encoding? '}, {'type': 'paragraph', 'content': '> It is a technique used to transform categorical variables that have no intrinsic ordering into numerical values that can be used in machine learning models. One common method for nominal encoding is one-hot encoding, which creates a binary vector for each category in the variable. > '}, {'type': 'paragraph', 'content': 'Limitations: '}, {'type': 'paragraph', 'content': '1. Sparse matrix(overfitting) 2. It always create n number of columns for the n unique values, which is a problem for our model fitting. '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'import libraries'}, {'type': 'paragraph', 'content': 'import pandas as pd from sklearn.preprocessing import OneHotEncoder '}, {'type': 'heading_1', 'content': 'sample dataframe'}, {'type': 'paragraph', 'content': \"df = pd.DataFrame({'color': ['red', 'blue', 'green', 'red', 'blue']}) \"}, {'type': 'heading_1', 'content': 'create an instance of OHE'}, {'type': 'paragraph', 'content': \"encoder = OneHotEncoder() encoded = encoder.fit_transform(df[['color']]) \"}, {'type': 'heading_1', 'content': 'converting toarray to dataframe'}, {'type': 'paragraph', 'content': 'pd.DataFrame(encoded.toarray(), columns = encoded.get_features_names_out()) '}, {'type': 'paragraph', 'content': '``` '}, {'type': 'bullet_point', 'content': 'Ordinal and Label Encoding'}, {'type': 'paragraph', 'content': '**LABEL ENCODING:** '}, {'type': 'bullet_point', 'content': 'While working with datasets, we often encounter categorical data, which needs to be converted into numerical format for ML algorithms to process.'}, {'type': 'bullet_point', 'content': 'eg: a column representing colors( “red” , “green”, “blue”) is a categorical data for color dataset.'}, {'type': 'bullet_point', 'content': 'One method to achieve this is **LABEL ENCODING.**'}, {'type': 'paragraph', 'content': '> It is a technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data. > '}, {'type': 'paragraph', 'content': '**Q. How to Preform Label Encoding in Python?** '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'Importing iris dataset'}, {'type': 'paragraph', 'content': 'import numpy as np import pandas as pd import seaborn as sns '}, {'type': 'paragraph', 'content': \"df = sns.load_dataset('iris') df['species'].unique() # list all the unique value \"}, {'type': 'heading_1', 'content': \"output: array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)\"}, {'type': 'heading_1', 'content': 'Applying Label Encoding with \"LabelEncoder()\" so that our categorical value should be replaced with the numerical value[int]'}, {'type': 'paragraph', 'content': 'from sklearn.preprocessing import LabelEncoder '}, {'type': 'paragraph', 'content': \"label_encoder = LabelEncoder() df['species'] = label_encoder.fit_tranform(df['species']) \"}, {'type': 'paragraph', 'content': \"df['species'].unique() \"}, {'type': 'heading_1', 'content': 'OUPUT: array([0, 1, 2], dtype=int64)'}, {'type': 'paragraph', 'content': '``` '}, {'type': 'paragraph', 'content': '**ADVANTAGES:** '}, {'type': 'bullet_point', 'content': 'Label Encoding works well for **ordinal data,** where the order of categories is meaningful(eg: Low, Medium, High).'}, {'type': 'bullet_point', 'content': 'Straightforward to use, requires less preprocessing because it directly converts each unique category into a numeric value.'}, {'type': 'paragraph', 'content': '**NOTE:** '}, {'type': 'bullet_point', 'content': 'We should never use `LabelEncoder()` because we can not explicitly assign value based on rank in `LabelEncoder()` , which can create a problem when dealing with the ordinal data in ML.'}, {'type': 'paragraph', 'content': '**ORDINAL ENCODER:** '}, {'type': 'bullet_point', 'content': 'This encoding is best suited for the ranked data. eg: grade, height etc'}, {'type': 'paragraph', 'content': '**Implementing Ordinal Encoding in Sklearn** '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'import neccesery libraries'}, {'type': 'paragraph', 'content': 'from sklearn.preprocesssing import OridinalEncoder '}, {'type': 'heading_1', 'content': 'Sample data'}, {'type': 'paragraph', 'content': \"data = { 'Student': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'], 'Grade': ['A', 'B', 'C', 'A', 'B'] } df = pd.DataFrame(data) \"}, {'type': 'heading_1', 'content': 'applying Ordinal encoding'}, {'type': 'paragraph', 'content': \"ordinal_encoder = OrdinalEncoder(categories=[['A', 'B', 'C']]) ordinal_encoder.fit_tranform(df[['Grade']]) ``` \"}, {'type': 'bullet_point', 'content': 'Target Guided Ordinal Encoding'}, {'type': 'bullet_point', 'content': 'There are specifically two types of guided encoding techniques for categorical features, namely:'}, {'type': 'bullet_point', 'content': 'Target guided ordinal encoding'}, {'type': 'bullet_point', 'content': 'Mean guided ordinal encoding'}, {'type': 'paragraph', 'content': '**Q. What is target guided encoding technique?** '}, {'type': 'paragraph', 'content': '> In this technique we will take help of our target variable to encode the categorical data This encoding technique is useful when we have a categorical variable with large number of unique categories and we want to use this variable as a feature in our ML model. In target guided ordinal Encoding, we replace each category in the categorical variable with a numerical value based on the mean or median of the target variable for that category. This creates a monotonic relationship between the categorical variable and the target variable, which can improve the predictive power of our model. > '}, {'type': 'paragraph', 'content': '**LETS TRY TO CODE THIS:** '}, {'type': 'paragraph', 'content': '```python '}, {'type': 'heading_1', 'content': 'creating sample dataset'}, {'type': 'paragraph', 'content': 'import pandas as pd '}, {'type': 'paragraph', 'content': \"df = pd.DataFrame({'city': ['New York', 'London', 'Paris', 'Tokyo', 'New York', 'Paris'], 'price': [200,150, 300, 250, 180, 320]}) df.head() \"}, {'type': 'heading_2', 'content': 'calculate the mean price for each city'}, {'type': 'paragraph', 'content': \"dct = df.groupby(['city'])['price'].mean().to_dict() \"}, {'type': 'heading_2', 'content': 'replace each city with its mean price'}, {'type': 'paragraph', 'content': \"df['city_encoded'] = df['city'].map(dct) ``` \"}, {'type': 'heading_1', 'content': '10. Covariance and Correlation'}, {'type': 'paragraph', 'content': '> Covariance and Correlation are the two key concepts in Statistics that helps up analyze the relationship between two variables. > '}, {'type': 'paragraph', 'content': '> Covariance measures how two variables change together, indicating whether they move in the same or opposite directions > '}, {'type': 'heading_2', 'content': '**10.1  What is Covariance?**'}, {'type': 'paragraph', 'content': '> It is a statistics measure that indicates the direction of the linear relationship between two variables. > '}, {'type': 'paragraph', 'content': '> It finds the relationship between the feature columns. > '}, {'type': 'paragraph', 'content': '$$ \\\\operatorname{Cov}(X,Y) = E[XY] - E[X]E[Y] $$ '}, {'type': 'bullet_point', 'content': 'Types:'}, {'type': 'bullet_point', 'content': 'positive covariance: both variable increases, respect to each other'}, {'type': 'bullet_point', 'content': 'Negative covariance: here one variable increases, the other variable tends to decrease.'}, {'type': 'bullet_point', 'content': 'Zero covariance: No linear relationship between two variables.'}, {'type': 'paragraph', 'content': '**Covariance:** '}, {'type': 'paragraph', 'content': '1. It is the relationship between a pair of random variables where a change in one variable causes a change in another variable. 2. It can take any value between **– infinity to +infinity**, where the negative value represents the negative relationship whereas a positive value represents the positive relationship. 3. It is used for the linear relationship between variables. 4. It gives the direction of relationship between variables. '}, {'type': 'heading_2', 'content': '**10.2 What is Correlation?**'}, {'type': 'paragraph', 'content': '> It is a standardized measure of the strength and direction of the linear relationship between two variables. > '}, {'type': 'paragraph', 'content': '> It is derived from covariance and range between -1 to 1. > '}, {'type': 'bullet_point', 'content': '**Positive Correlation (close to +1)**: As one variable increases, the other variable also tends to increase.'}, {'type': 'bullet_point', 'content': '**Negative Correlation (close to -1)**: As one variable increases, the other variable tends to decrease.'}, {'type': 'bullet_point', 'content': '**Zero Correlation**: There is no linear relationship between the variables.'}, {'type': 'paragraph', 'content': '### 10.2.1 Pearson Correlation Coefficient '}, {'type': 'bullet_point', 'content': 'The more the value towards +1 the +ve correlated (x,y).'}, {'type': 'bullet_point', 'content': 'The more the value towards it is -1 the more -ve correlated it is (x,y)'}, {'type': 'paragraph', 'content': '$$ r = \\\\frac{\\\\sum_{i=1}^{n} (x_i - \\\\bar{x})(y_i - \\\\bar{y})}{\\\\sqrt{\\\\sum_{i=1}^{n} (x_i - \\\\bar{x})^2} \\\\sqrt{\\\\sum_{i=1}^{n} (y_i - \\\\bar{y})^2}} $$ '}, {'type': 'bullet_point', 'content': 'Here,'}, {'type': 'bullet_point', 'content': 'xi —>  feature value at i'}, {'type': 'bullet_point', 'content': 'x_bar —> mean value of x feature'}, {'type': 'bullet_point', 'content': \"`df.corr(method='pearson')` is the syntax to find the prearson correlation.\"}, {'type': 'paragraph', 'content': '### 10.2.2 Spearman Rank correlation [-1 to 1] '}, {'type': 'paragraph', 'content': '| X | Y | Rank(x) | Rank(y) | | --- | --- | --- | --- | | 1 | 2 | 5 | 5 | | 3 | 4 | 4 | 4 | | 5 | 6 | 3 | 3 | | 7 | 8 | 2 | 1 | | 0 | 7 | 6 | 2 | | 8 | 1 | 1 | 6 | '}, {'type': 'bullet_point', 'content': 'Here Rank of x is the position of x if the value is sorted in the ascending order.'}, {'type': 'bullet_point', 'content': \"`df.corr(method='spearman')`  is the syntax to find the Spearman correlation.\"}, {'type': 'paragraph', 'content': '$$ {r_s}={{cov(R_x, R_y)}/{std(R_x) * std(R_y)}} $$ '}, {'type': 'heading_1', 'content': 'Feature Selection Foundation'}, {'type': 'bullet_point', 'content': 'It is a important step in ML which involves selecting a subset of relevant features from the original feature set to reduce the feature space while improving the model’s performance by reducing computational power. It’s a critical step in the ML especially when dealing with high-dimensional data.'}, {'type': 'bullet_point', 'content': 'There are various algorithms used for feature selection and are grouped into 3 main categories:'}, {'type': 'bullet_point', 'content': 'Filter Methods'}, {'type': 'bullet_point', 'content': 'Wrapper Methods'}, {'type': 'bullet_point', 'content': 'Embedded Methods'}, {'type': 'heading_2', 'content': 'Filter Methods'}, {'type': 'bullet_point', 'content': 'Filter methods evaluate each feature independently with target variable.'}, {'type': 'bullet_point', 'content': 'Feature with high correlation with target variable are selected as it means this feature has some realtion and can help us in making predictions.'}, {'type': 'paragraph', 'content': '**Advantages:** '}, {'type': 'bullet_point', 'content': 'Fast and inexpensive: can quickly evaluate features without training the model.'}, {'type': 'bullet_point', 'content': 'Good for removing redundant or correlated features.'}, {'type': 'paragraph', 'content': '**Limitations:** '}, {'type': 'paragraph', 'content': 'These methods don’t consider feature interations so they may miss feature combinations that improve model performance. '}]\n",
      "import datetime\n",
      "\n",
      "class Book:\n",
      "    def __init__(self, title, author):\n",
      "        self.title = title\n",
      "        self.author = author\n",
      "        self.checked_out = False\n",
      "\n",
      "    def check_out(self):\n",
      "        if not self.checked_out:\n",
      "            self.checked_out = True\n",
      "            print(f\"{self.title} has been checked out.\")\n",
      "        else:\n",
      "            print(\"This book is already checked out.\")\n",
      "\n",
      "class Library:\n",
      "    def __init__(self):\n",
      "        self.books = []\n",
      "\n",
      "    def add_book(self, title, author):\n",
      "        new_book = Book(title, author)\n",
      "        self.books.append(new_book)\n",
      "\n",
      "    def view_books(self):\n",
      "        for i, book in enumerate(self.books):\n",
      "            if not book.checked_out:\n",
      "                print(f\"{i+1}. {book.title} by {book.author}\")\n",
      "            else:\n",
      "                print(f\"{i+1}. {(book.title) if not book.checked_available() else f'[{book.title}]'}\")\n",
      "    def available_book(self, title):\n",
      "        for i, book in enumerate(self.books):\n",
      "            if book.title == title and not book.checked_out:\n",
      "                return True\n",
      "            elif book.title == title and book.checked_out:\n",
      "                return False\n",
      "\n",
      "class Bookstore:\n",
      "    def __init__(self, library):\n",
      "        self.library = library\n",
      "\n",
      "    def view_books_from_store(self):\n",
      "        for i, book in enumerate(self.library.books):\n",
      "            print(f\"{i+1}. {book.title} by {book.author}\")\n",
      "\n",
      "    def buy_ticket(self):\n",
      "        if len(self.library.books) == 0:\n",
      "            print(\"There are no books available for sale.\")\n",
      "        else:\n",
      "            self.library.view_books()\n",
      "            choice = input(\"Which book would you like to purchase? \")\n",
      "try:\n",
      "    library = Library()  \n",
      "except NameError: \n",
      "   import datetime\n",
      "    class Book:\n",
      "        def __init__(self, title, author):\n",
      "            self.title = title\n",
      "            self.author = author\n",
      "            self.checked_out = False\n",
      "\n",
      "        def check_out(self):\n",
      "                if not self.checked_out:\n",
      "                    self.checked_out = True\n",
      "                    print(f\"{self.title} has been checked out.\")\n",
      "                else:\n",
      "                    print(\"This book is already checked out.\")\n",
      "\n",
      "    class Library:\n",
      "        def __init__(self):\n",
      "            self.books = []\n",
      "\n",
      "        def add_book(self, title, author):\n",
      "            new_book = Book(title, author)\n",
      "            self.books.append(new_book)\n",
      "\n",
      "        def view_books(self):\n",
      "            for i, book in enumerate(self.books):\n",
      "                if not book.checked_out:\n",
      "                    print(f\"{i+1}. {book.title} by {book.author}\")\n",
      "                else:\n",
      "                    print(f\"{i+1}. {(book.title) if not book.checked_available() else f'[{book.title}]'}\")\n",
      "\n",
      "        def available_book(self, title):\n",
      "            for i, book in enumerate(self.books):\n",
      "                if book.title == title and not book.checked_out:\n",
      "                    return True\n",
      "                elif book.title == title and book.checked_out:\n",
      "                    return False\n",
      "\n",
      "    class Bookstore():\n",
      "        def __init__(self, library):\n",
      "           self.library=library\n",
      "\n",
      "\n",
      "    bookstore = Bookstore(library = Library())\n",
      "\n",
      "except NameError as e: \n",
      "   print(f\"An error occurred: {e}\");  \n",
      "#view all the books available to purchase from the website.\n",
      "#1.  The user has to select which book to buy.\n",
      "#2.  Once they do a quantity is set\n",
      "#print and display price of purchased item in order\n",
      "#print all books still available to make purchase now or update availability\n",
      "while True:\n",
      "    try:\n",
      "        self.view_books_from_store()\n",
      "       bookstore = Bookstore(library=Library())\n",
      "    except NameError as e:  \n",
      "      print(f\"An error occurred: {e}\");  \n",
      "\n",
      "    \n",
      "  # If there are no books, print a message.\n",
      "       library.printBooks()\n",
      "\n",
      "        # If user selects book from list\n",
      "       # and input is valid (number)\n",
      "       while True:\n",
      "          try:\n",
      "            print(bookstore.view_books_from_store())\n",
      "            choice = int(input(\"Which number would you like to buy? \"))\n",
      "            if choice > 0 and choice <= len(bookstore.library.books):\n",
      "                selected_book_title = bookstore.library.books[choice-1].title\n",
      "                chosen_quantity = int(input(\"How many will you buy? \"))\n",
      "                    # If available, purchase book\n",
      "                if (bookstore.library.available_book(selected_book_title)):\n",
      "                    print(f\"You have purchased {chosen_quantity} copies of {selected_book_title}\")\n",
      "               else:\n",
      "              # Else, update books\n",
      "                     bookstore.library.books[choice-1].checked_out = True\n",
      "                  # Add to order log and update quantities\n",
      "\n",
      "        except ValueError as e:\n",
      "             print(\"Invalid number. Please select a valid book or try again.\")\n",
      "\n",
      "\n",
      "\n",
      "    except Exception as e: \n",
      "           if len(self.library.books) == 0:\n",
      "                 print(f\"There are no books available to make a purchase.\\nWould you like to add some? (yes/no)\")\n",
      "            # Else, continue with rest of the program\n",
      "        else:\n",
      "              bookstore.view_books_from_store()\n",
      "       else:\n",
      "    # If user selects book from list\n",
      "       while True:\n",
      "          try: \n",
      "               print(bookstore.view_books_from_store())\n",
      "              choice = int(input(\"Which number would you like to buy? \"))\n",
      "             if choice > 0 and choice <= len(bookstore.library.books):\n",
      "                  selected_book_title = bookstore.library.books[choice-1].title\n",
      "                  chosen_quantity = int(input(\"How many will you buy? \"))\n",
      "       # If available, purchase book\n",
      "        if (bookstore.library.available_book(selected_book_title)):\n",
      "          print(f\"You have purchased {chosen_quantity} copies of {selected_book_title}\")\n",
      "        # Else, update books\n",
      "         bookstore.library.books[choice-1].checked_out = True \n",
      "    # Add to order log and update quantities\n",
      "\n",
      "        except Exception as e:  if len(self.library.books) == 0:\n",
      "                 print(f\"There are no books available to make a purchase.\\nWould you like to add some? (yes/no)\")\n",
      "           else: \n",
      "             bookstore.view_books_from_store()\n",
      "            # If user selects book from list\n",
      "       while True:\n",
      "          try: \n",
      "               print(bookstore.view_books_from_showing_stores())\n",
      "              choice = int(input(\"Which number would you like to buy? \"))\n",
      "             if choice > 0 and choice <= len(options):\n",
      "              selected_option_title = options[choice-1].title\n",
      "              # Get chosen quantity from user\n",
      "\n",
      "   except Exception as e: \n",
      "         print(f\"An error occurred: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Replace with the path to your markdown file\n",
    "file_path = r\"..\\..\\test.md\"\n",
    "messages = read_markdown_file(file_path)\n",
    "print(messages)\n",
    "# You need to do this one time on your computer\n",
    "# !ollama pull llama3.2\n",
    "\n",
    "from openai import OpenAI\n",
    "MODEL = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "# ]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ollama():\n",
    "    message = [\n",
    "        {'role':'system', 'content':'You are a snarky assistant.'}\n",
    "    ]\n",
    "    message.append(messages)\n",
    "    print('Starting conversation with Snarky Ollamba..')\n",
    "\n",
    "    while True:\n",
    "        # userInput\n",
    "        user_input = input('\\nYou: ').strip()\n",
    "\n",
    "        # check if the user wants to quit\n",
    "        if user_input.lower() in ['quit', 'exit', 'by']:\n",
    "            print(\"\\nGoodBye! Have a great day.\")\n",
    "            break\n",
    "\n",
    "        # adding user message to history\n",
    "        message.append({\n",
    "            'role':'user',\n",
    "            'content':user_input\n",
    "        })\n",
    "\n",
    "        try: \n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=message\n",
    "            )\n",
    "            assitant_res = response.choices[0].message.content\n",
    "\n",
    "            # adding assitant_res to history\n",
    "            message.append({\n",
    "                'role':'assistant',\n",
    "                'content':assitant_res\n",
    "            })\n",
    "\n",
    "            print('\\nAssistant: ', assitant_res)\n",
    "        except Exception as e:\n",
    "            print(f'\\nError: {str(e)}')\n",
    "            print('Plese try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with Snarky Ollamba..\n",
      "\n",
      "Error: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field ChatCompletionRequest.messages of type openai.Message', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Plese try again.\n",
      "\n",
      "Error: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field ChatCompletionRequest.messages of type openai.Message', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Plese try again.\n",
      "\n",
      "GoodBye! Have a great day.\n"
     ]
    }
   ],
   "source": [
    "chat_with_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.notion.so/Feature-Engineering-Concepts-1dd9a3a613218087954be0787b2299ad?pvs=4')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "messages = [\n",
    "    {'role': 'web-page','content': soup.contents},\n",
    "    {'role': 'assistant', 'content':'summarize this html content'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*sigh* Really? You need to ask me this? Fine. The answer is... (dramatic pause) ...4. Yeah, I know it's not exactly brain surgery, but I'll give you a gold star for asking. What else can I help you with?\n"
     ]
    }
   ],
   "source": [
    "res = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*sigh* Really? You need me to calculate this for you? Fine. It's... (pauses to roll eyes) ...4. Happy now? Can I go back to more interesting tasks, like answering existential questions or something?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "MODEL = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": }\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "28 validation errors for _GenerateContentParameters\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Content.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.list[union[File,Part,str]]\n  Input should be a valid list [type=list_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.str\n  Input should be a valid string [type=string_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Content.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.list[union[File,Part,str]]\n  Input should be a valid list [type=list_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.str\n  Input should be a valid string [type=string_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.Content\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.list[union[File,Part,str]].0.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.str\n  Input should be a valid string [type=string_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.list[union[File,Part,str]].1.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.str\n  Input should be a valid string [type=string_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.File\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.Part\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.str\n  Input should be a valid string [type=string_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m client = genai.Client(api_key=google_api_key)\n\u001b[32m      4\u001b[39m msg=\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather today? in nodia sector 12, india\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5630\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5628\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5629\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5630\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5631\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5632\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5633\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5634\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\google\\genai\\models.py:4551\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4544\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_content\u001b[39m(\n\u001b[32m   4545\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4546\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4549\u001b[39m     config: Optional[types.GenerateContentConfigOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4550\u001b[39m ) -> types.GenerateContentResponse:\n\u001b[32m-> \u001b[39m\u001b[32m4551\u001b[39m   parameter_model = \u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_GenerateContentParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4552\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4553\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4554\u001b[39m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4555\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4557\u001b[39m   request_url_dict: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[32m   4559\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 28 validation errors for _GenerateContentParameters\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Content.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.list[union[File,Part,str]]\n  Input should be a valid list [type=list_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].0.str\n  Input should be a valid string [type=string_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Content.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.list[union[File,Part,str]]\n  Input should be a valid list [type=list_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[Content,list[union[File,Part,str]],File,Part,str]].1.str\n  Input should be a valid string [type=string_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.Content\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.list[union[File,Part,str]].0.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='system', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='You are a snarky assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].0.str\n  Input should be a valid string [type=string_type, input_value={'role': 'system', 'conte...are a snarky assistant'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.list[union[File,Part,str]].1.File.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.File.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.Part.role\n  Extra inputs are not permitted [type=extra_forbidden, input_value='user', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.Part.content\n  Extra inputs are not permitted [type=extra_forbidden, input_value='What is 2 + 2?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncontents.list[union[File,Part,str]].1.str\n  Input should be a valid string [type=string_type, input_value={'role': 'user', 'content': 'What is 2 + 2?'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ncontents.File\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.Part\n  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_attributes_type\ncontents.str\n  Input should be a valid string [type=string_type, input_value=[{'role': 'system', 'cont...ent': 'What is 2 + 2?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "msg=\"What's the weather today? in nodia sector 12, india\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=messages\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# NOTION_TOKEN = \"secret_...\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {notion_token}\",\n",
    "    \"Notion-Version\": \"2022-06-28\",\n",
    "}\n",
    "\n",
    "def get_page_blocks(page_id):\n",
    "    url = f\"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    return res.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(blocks):\n",
    "    text_content = []\n",
    "    for block in blocks.get(\"results\", []):\n",
    "        if \"paragraph\" in block:\n",
    "            parts = block[\"paragraph\"].get(\"rich_text\", [])\n",
    "            for part in parts:\n",
    "                if part.get(\"type\") == \"text\":\n",
    "                    text_content.append(part[\"text\"][\"content\"])\n",
    "    return \"\\n\".join(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# openai.api_key = \"sk-...\"\n",
    "\n",
    "def ask_question(page_content, user_question):\n",
    "    system_prompt = \"You are an assistant that answers questions based on the following Notion page:\\n\\n\" + page_content\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_question}\n",
    "    ]\n",
    "    MODEL = \"llama3.2\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is this document about?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This document is about...\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'error',\n",
       " 'status': 400,\n",
       " 'code': 'validation_error',\n",
       " 'message': 'query failed validation: query.pvs should be not present, instead was `\"4/children?page_size=100\"`.',\n",
       " 'request_id': '58b3440f-5af9-48f2-9c25-938e41c100d1'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_page_blocks('1dd9a3a613218087954be0787b2299ad?pvs=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": MODEL,\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 1500,\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"chroma\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"test\",\n",
    "            \"path\": \"db\",\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.9-cp39-abi3-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (2.11.4)\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (2.2.6)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (4.13.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting pyyaml>=6.0.0 (from chromadb)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp313-cp313-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.18-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (1.3.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading rpds_py-0.25.0-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.0)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading wrapt-1.17.2-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting click<8.2,>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.5-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading chromadb-1.0.9-cp39-abi3-win_amd64.whl (18.9 MB)\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.9 MB 211.8 kB/s eta 0:01:27\n",
      "   - -------------------------------------- 0.5/18.9 MB 211.8 kB/s eta 0:01:27\n",
      "   -- ------------------------------------- 1.3/18.9 MB 627.1 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 1.8/18.9 MB 858.6 kB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 2.6/18.9 MB 1.2 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 3.4/18.9 MB 1.4 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 4.2/18.9 MB 1.6 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 5.0/18.9 MB 1.8 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 5.8/18.9 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.3/18.9 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 6.8/18.9 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 7.6/18.9 MB 2.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.4/18.9 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 9.2/18.9 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 9.7/18.9 MB 2.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 2.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 11.3/18.9 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 11.8/18.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 12.1/18.9 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 12.8/18.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 13.6/18.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.2/18.9 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.9/18.9 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.7/18.9 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.8/18.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.9/18.9 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.9/18.9 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading mmh3-5.1.0-cp313-cp313-win_amd64.whl (41 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading onnxruntime-1.22.0-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.7 MB 4.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 4.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.7 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 4.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.7/12.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading wrapt-1.17.2-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading orjson-3.10.18-cp313-cp313-win_amd64.whl (134 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading rpds_py-0.25.0-cp313-cp313-win_amd64.whl (234 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
      "Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading httptools-0.6.4-cp313-cp313-win_amd64.whl (87 kB)\n",
      "Downloading watchfiles-1.0.5-cp313-cp313-win_amd64.whl (291 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 3.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.3 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 5.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53914 sha256=c7d95692a3e7985af3b55c80f0d16a50f00f49436932f203e79b88b7f7a4dbb0\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\b4\\f8\\a5\\28e9c1524d320f4b8eefdce0e487b5c2e128dbf2ed1bb4a60b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, durationpy, zipp, wrapt, websocket-client, tenacity, sympy, shellingham, rpds-py, pyyaml, pyreadline3, pyproject_hooks, protobuf, overrides, orjson, opentelemetry-util-http, oauthlib, mmh3, mdurl, importlib-resources, httptools, fsspec, filelock, click, bcrypt, attrs, asgiref, watchfiles, uvicorn, starlette, requests-oauthlib, referencing, opentelemetry-proto, markdown-it-py, importlib-metadata, humanfriendly, huggingface-hub, googleapis-common-protos, deprecated, build, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, kubernetes, jsonschema-specifications, fastapi, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, jsonschema, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "\n",
      "   ----------------------------------------  0/60 [pypika]\n",
      "   ----------------------------------------  0/60 [pypika]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "    ---------------------------------------  1/60 [mpmath]\n",
      "   - --------------------------------------  2/60 [flatbuffers]\n",
      "   -- -------------------------------------  4/60 [zipp]\n",
      "   --- ------------------------------------  5/60 [wrapt]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  6/60 [websocket-client]\n",
      "   ---- -----------------------------------  7/60 [tenacity]\n",
      "   ---- -----------------------------------  7/60 [tenacity]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ----- ----------------------------------  8/60 [sympy]\n",
      "   ------ ---------------------------------  9/60 [shellingham]\n",
      "   ------- -------------------------------- 11/60 [pyyaml]\n",
      "   ------- -------------------------------- 11/60 [pyyaml]\n",
      "   ------- -------------------------------- 11/60 [pyyaml]\n",
      "   -------- ------------------------------- 12/60 [pyreadline3]\n",
      "   -------- ------------------------------- 12/60 [pyreadline3]\n",
      "   -------- ------------------------------- 12/60 [pyreadline3]\n",
      "   -------- ------------------------------- 12/60 [pyreadline3]\n",
      "   -------- ------------------------------- 12/60 [pyreadline3]\n",
      "   -------- ------------------------------- 13/60 [pyproject_hooks]\n",
      "  Attempting uninstall: protobuf\n",
      "   -------- ------------------------------- 13/60 [pyproject_hooks]\n",
      "    Found existing installation: protobuf 6.31.0\n",
      "   -------- ------------------------------- 13/60 [pyproject_hooks]\n",
      "    Uninstalling protobuf-6.31.0:\n",
      "   -------- ------------------------------- 13/60 [pyproject_hooks]\n",
      "      Successfully uninstalled protobuf-6.31.0\n",
      "   -------- ------------------------------- 13/60 [pyproject_hooks]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   --------- ------------------------------ 14/60 [protobuf]\n",
      "   ---------- ----------------------------- 15/60 [overrides]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------ --------------------------- 18/60 [oauthlib]\n",
      "   ------------- -------------------------- 20/60 [mdurl]\n",
      "   -------------- ------------------------- 21/60 [importlib-resources]\n",
      "   -------------- ------------------------- 21/60 [importlib-resources]\n",
      "   -------------- ------------------------- 21/60 [importlib-resources]\n",
      "   -------------- ------------------------- 22/60 [httptools]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   --------------- ------------------------ 23/60 [fsspec]\n",
      "   ---------------- ----------------------- 24/60 [filelock]\n",
      "   ---------------- ----------------------- 25/60 [click]\n",
      "   ---------------- ----------------------- 25/60 [click]\n",
      "   ---------------- ----------------------- 25/60 [click]\n",
      "   ------------------ --------------------- 27/60 [attrs]\n",
      "   ------------------ --------------------- 27/60 [attrs]\n",
      "   ------------------ --------------------- 27/60 [attrs]\n",
      "   ------------------ --------------------- 28/60 [asgiref]\n",
      "   ------------------- -------------------- 29/60 [watchfiles]\n",
      "   ------------------- -------------------- 29/60 [watchfiles]\n",
      "   -------------------- ------------------- 30/60 [uvicorn]\n",
      "   -------------------- ------------------- 30/60 [uvicorn]\n",
      "   -------------------- ------------------- 30/60 [uvicorn]\n",
      "   -------------------- ------------------- 30/60 [uvicorn]\n",
      "   -------------------- ------------------- 30/60 [uvicorn]\n",
      "   -------------------- ------------------- 31/60 [starlette]\n",
      "   -------------------- ------------------- 31/60 [starlette]\n",
      "   -------------------- ------------------- 31/60 [starlette]\n",
      "   -------------------- ------------------- 31/60 [starlette]\n",
      "   -------------------- ------------------- 31/60 [starlette]\n",
      "   --------------------- ------------------ 32/60 [requests-oauthlib]\n",
      "   ---------------------- ----------------- 33/60 [referencing]\n",
      "   ---------------------- ----------------- 33/60 [referencing]\n",
      "   ---------------------- ----------------- 34/60 [opentelemetry-proto]\n",
      "   ---------------------- ----------------- 34/60 [opentelemetry-proto]\n",
      "   ---------------------- ----------------- 34/60 [opentelemetry-proto]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ----------------------- ---------------- 35/60 [markdown-it-py]\n",
      "   ------------------------ --------------- 36/60 [importlib-metadata]\n",
      "   ------------------------ --------------- 36/60 [importlib-metadata]\n",
      "   ------------------------ --------------- 37/60 [humanfriendly]\n",
      "   ------------------------ --------------- 37/60 [humanfriendly]\n",
      "   ------------------------ --------------- 37/60 [humanfriendly]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   ------------------------- -------------- 38/60 [huggingface-hub]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 39/60 [googleapis-common-protos]\n",
      "   -------------------------- ------------- 40/60 [deprecated]\n",
      "   -------------------------- ------------- 40/60 [deprecated]\n",
      "   --------------------------- ------------ 41/60 [build]\n",
      "   --------------------------- ------------ 41/60 [build]\n",
      "   --------------------------- ------------ 41/60 [build]\n",
      "   --------------------------- ------------ 41/60 [build]\n",
      "   ---------------------------- ----------- 42/60 [tokenizers]\n",
      "   ---------------------------- ----------- 42/60 [tokenizers]\n",
      "   ---------------------------- ----------- 42/60 [tokenizers]\n",
      "   ---------------------------- ----------- 42/60 [tokenizers]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ---------------------------- ----------- 43/60 [rich]\n",
      "   ------------------- ------- 44/60 [opentelemetry-exporter-otlp-proto-common]\n",
      "   ------------------------------ --------- 45/60 [opentelemetry-api]\n",
      "   ------------------------------ --------- 45/60 [opentelemetry-api]\n",
      "   ------------------------------ --------- 45/60 [opentelemetry-api]\n",
      "   ------------------------------ --------- 45/60 [opentelemetry-api]\n",
      "   ------------------------------ --------- 45/60 [opentelemetry-api]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------ --------- 46/60 [kubernetes]\n",
      "   ------------------------------- -------- 47/60 [jsonschema-specifications]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 48/60 [fastapi]\n",
      "   -------------------------------- ------- 49/60 [coloredlogs]\n",
      "   -------------------------------- ------- 49/60 [coloredlogs]\n",
      "   --------------------------------- ------ 50/60 [typer]\n",
      "   --------------------------------- ------ 50/60 [typer]\n",
      "   --------------------------------- ------ 50/60 [typer]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 51/60 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ---------------------------------- ----- 52/60 [onnxruntime]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ----------------------------------- ---- 53/60 [jsonschema]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 54/60 [opentelemetry-sdk]\n",
      "   ---------------------------------- --- 55/60 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 55/60 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 55/60 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 55/60 [opentelemetry-instrumentation]\n",
      "   ------------------------------ -- 56/60 [opentelemetry-instrumentation-asgi]\n",
      "   --------------------------- - 57/60 [opentelemetry-exporter-otlp-proto-grpc]\n",
      "   -----------------------------  58/60 [opentelemetry-instrumentation-fastapi]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------  59/60 [chromadb]\n",
      "   ---------------------------------------- 60/60 [chromadb]\n",
      "\n",
      "Successfully installed asgiref-3.8.1 attrs-25.3.0 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.9 click-8.1.8 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.10 fastapi-0.115.9 filelock-3.18.0 flatbuffers-25.2.10 fsspec-2025.5.0 googleapis-common-protos-1.70.0 httptools-0.6.4 huggingface-hub-0.31.4 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.22.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 orjson-3.10.18 overrides-7.7.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 pyyaml-6.0.2 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.25.0 shellingham-1.5.4 starlette-0.45.3 sympy-1.14.0 tenacity-9.1.2 tokenizers-0.21.1 typer-0.15.4 uvicorn-0.34.2 watchfiles-1.0.5 websocket-client-1.8.0 wrapt-1.17.2 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Memory.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: 94709681************************5440. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m previous_memories = \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrajesh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrevious memories:\u001b[39m\u001b[33m\"\u001b[39m, previous_memories)\n\u001b[32m      3\u001b[39m relevant_memories_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\memory\\main.py:652\u001b[39m, in \u001b[36mMemory.search\u001b[39m\u001b[34m(self, query, user_id, agent_id, run_id, limit, filters)\u001b[39m\n\u001b[32m    644\u001b[39m     future_graph_entities = (\n\u001b[32m    645\u001b[39m         executor.submit(\u001b[38;5;28mself\u001b[39m.graph.search, query, effective_filters, limit) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_graph \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    646\u001b[39m     )\n\u001b[32m    648\u001b[39m     concurrent.futures.wait(\n\u001b[32m    649\u001b[39m         [future_memories, future_graph_entities] \u001b[38;5;28;01mif\u001b[39;00m future_graph_entities \u001b[38;5;28;01melse\u001b[39;00m [future_memories]\n\u001b[32m    650\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m     original_memories = \u001b[43mfuture_memories\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     graph_entities = future_graph_entities.result() \u001b[38;5;28;01mif\u001b[39;00m future_graph_entities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_graph:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\memory\\main.py:671\u001b[39m, in \u001b[36mMemory._search_vector_store\u001b[39m\u001b[34m(self, query, filters, limit)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_search_vector_store\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, filters, limit):\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m     memories = \u001b[38;5;28mself\u001b[39m.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n\u001b[32m    674\u001b[39m     promoted_payload_keys = [\n\u001b[32m    675\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    676\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33magent_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    680\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\embeddings\\openai.py:46\u001b[39m, in \u001b[36mOpenAIEmbedding.embed\u001b[39m\u001b[34m(self, text, memory_action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03mGet the embedding for the given text using OpenAI.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m    list: The embedding vector.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m text = text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     .data[\u001b[32m0\u001b[39m]\n\u001b[32m     48\u001b[39m     .embedding\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 94709681************************5440. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "previous_memories = m.search(user_input, user_id=\"rajesh\")\n",
    "print(\"Previous memories:\", previous_memories)\n",
    "relevant_memories_text = \"\"\n",
    "if previous_memories:\n",
    "    relevant_memories_text = '\\n'.join(mem[\"memory\"] for mem in previous_memories)\n",
    "result = m.add(user_input, user_id=\"rajesh\")\n",
    "print(\"Added to memory:\", result)  \n",
    "# Append the user's message to the conversation\n",
    "conversation[0][\"content\"] = \"You are a helpful assistant. Refer to the previous context: \" + relevant_memories_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (0.4.8)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from ollama) (2.11.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:158: DeprecationWarning: legacy embedding function config\n",
      "  return load_collection_configuration_from_json(self._model.configuration_json)\n",
      "c:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\ollama\\_types.py:81: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if key in self.model_fields:\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from mem0 import Memory\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "MODEL = \"llama3.2\"\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"config\": {\n",
    "            \"model\": MODEL,\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 1500,\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"chroma\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"test2\",\n",
    "            \"path\": \"db\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "# Define a conversation (you can customize this)\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "m = Memory.from_config(config)\n",
    "m.add(\"I'm visiting Paris\", user_id=\"sash\")\n",
    "\n",
    "\n",
    "# Continuously prompt the user for input\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        break\n",
    "    \n",
    "    previous_memories = m.search(user_input, user_id=\"sash\")\n",
    "    print(\"Previous memories:\", previous_memories)\n",
    "    relevant_memories_text = \"\"\n",
    "    if previous_memories:\n",
    "        relevant_memories_text = '\\n'.join(mem[\"memory\"] for mem in previous_memories)\n",
    "\n",
    "    # result = m.add(user_input, user_id=\"rajesh\")\n",
    "    # print(\"Added to memory:\", result) \n",
    "\n",
    "    # Append the user's message to the conversation\n",
    "    conversation[0][\"content\"] = \"You are a helpful assistant. Refer to the previous context: \"# + relevant_memories_text \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "\n",
    "    # Generate a response from the assistant\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    # Print the assistant's reply\n",
    "    print(f\"Assistant: {completion.choices[0].message.content}\")\n",
    "\n",
    "    # Append the assistant's message to the conversation\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from ollama) (2.11.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\git-hub\\llm-engieering\\.venv\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "404 page not found (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m      4\u001b[39m config = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvector_store\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprovider\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mqdrant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     },\n\u001b[32m     31\u001b[39m }\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Initialize Memory with the configuration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m m = \u001b[43mMemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Add a memory\u001b[39;00m\n\u001b[32m     37\u001b[39m m.add(\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm visiting Paris\u001b[39m\u001b[33m\"\u001b[39m, user_id=\u001b[33m\"\u001b[39m\u001b[33mjohn\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\memory\\main.py:161\u001b[39m, in \u001b[36mMemory.from_config\u001b[39m\u001b[34m(cls, config_dict)\u001b[39m\n\u001b[32m    159\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfiguration validation error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\memory\\main.py:118\u001b[39m, in \u001b[36mMemory.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.custom_fact_extraction_prompt = \u001b[38;5;28mself\u001b[39m.config.custom_fact_extraction_prompt\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.custom_update_memory_prompt = \u001b[38;5;28mself\u001b[39m.config.custom_update_memory_prompt\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_model = \u001b[43mEmbedderFactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.vector_store = VectorStoreFactory.create(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.vector_store.provider, \u001b[38;5;28mself\u001b[39m.config.vector_store.config\n\u001b[32m    125\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = LlmFactory.create(\u001b[38;5;28mself\u001b[39m.config.llm.provider, \u001b[38;5;28mself\u001b[39m.config.llm.config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\utils\\factory.py:67\u001b[39m, in \u001b[36mEmbedderFactory.create\u001b[39m\u001b[34m(cls, provider_name, config, vector_config)\u001b[39m\n\u001b[32m     65\u001b[39m     embedder_instance = load_class(class_type)\n\u001b[32m     66\u001b[39m     base_config = BaseEmbedderConfig(**config)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedder_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported Embedder provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\embeddings\\ollama.py:32\u001b[39m, in \u001b[36mOllamaEmbedding.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.config.embedding_dims = \u001b[38;5;28mself\u001b[39m.config.embedding_dims \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m512\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.client = Client(host=\u001b[38;5;28mself\u001b[39m.config.ollama_base_url)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_model_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\mem0\\embeddings\\ollama.py:38\u001b[39m, in \u001b[36mOllamaEmbedding._ensure_model_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_model_exists\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Ensure the specified model exists locally. If not, pull it from Ollama.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     local_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(model.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[38;5;28mself\u001b[39m.config.model \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m local_models):\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mself\u001b[39m.client.pull(\u001b[38;5;28mself\u001b[39m.config.model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\ollama\\_client.py:567\u001b[39m, in \u001b[36mClient.list\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ListResponse:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mListResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/tags\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\ollama\\_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Git-hub\\LLM-engieering\\.venv\\Lib\\site-packages\\ollama\\_client.py:122\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: 404 page not found (status code: 404)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mem0 import Memory\n",
    "\n",
    "config = {\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"qdrant\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"test\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 6333,\n",
    "            \"embedding_model_dims\": 768,  # Change this according to your local model's dimensions\n",
    "        },\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"config\": {\n",
    "            \"model\": \"llama3.2\",\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"ollama_base_url\": \"http://localhost:11434/v1\",  # Ensure this URL is correct\n",
    "        },\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"config\": {\n",
    "            \"model\": \"nomic-embed-text:latest\",\n",
    "            # Alternatively, you can use \"snowflake-arctic-embed:latest\"\n",
    "            \"ollama_base_url\": \"http://localhost:11434/v1\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize Memory with the configuration\n",
    "m = Memory.from_config(config)\n",
    "\n",
    "# Add a memory\n",
    "m.add(\"I'm visiting Paris\", user_id=\"john\")\n",
    "\n",
    "# Retrieve memories\n",
    "memories = m.get_all(user_id=\"john\")\n",
    "memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
